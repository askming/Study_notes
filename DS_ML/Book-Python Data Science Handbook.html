
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>[book] Python Data Science Handbook &#8212; Study Notes</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="[book] Hands-On Machine Learning with Scikit-Learn and TensorFlow" href="Book-Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Study Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Study Notes Index
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Analysis%20of%20Observational%20Health%20Care%20Data%20Using%20SAS.html">
   [book] Analysis of Observational Health Care Data Using SAS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Causal%20inference%20in%20statistics%20a%20primer.html">
   [book] Causal inference in statistics
   <em>
    a primer
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Data%20Analysis%20Using%20Regression%20and%20Multilevel%20Hierarchical%20Models.html">
   [book] Data Analysis Using Regression and Multilevel/Hierarchical Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Fundamentals%20of%20Clinical%20Trials%20%284th%20edition%29.html">
   [book] Fundamentals of Clinical Trials (4th edition)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Regression%20Modeling%20Strategies.html">
   [book] Regression Modeling Strategies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Competing_Risk_Regression.html">
   Competing risk models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Cox_PH_model.html">
   ​Cox PH model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Survival_Analysis.html">
   Survival Analysis
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical computing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats_Comp/How%20R%20searches%20and%20finds%20stuff.html">
   How R searches and finds stuff
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  DS/ML
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Book-Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow.html">
   [book] Hands-On Machine Learning with Scikit-Learn and TensorFlow
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   [book] Python Data Science Handbook
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/DS_ML/Book-Python Data Science Handbook.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/askming/study_notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/askming/study_notes/issues/new?title=Issue%20on%20page%20%2FDS_ML/Book-Python Data Science Handbook.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#numpy">
   1. Numpy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-types-in-python-snake">
     Data types in Python :snake:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basics-of-numpy-arrays">
     Basics of NumPy arrays
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-on-numpy-arrays-universal-functions">
     Computation on NumPy Arrays: Universal Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aggregations-min-max-and-everything-in-between">
     Aggregations: Min, Max, and Everything In Between
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computation-on-arrays-broadcasting">
     Computation on Arrays: Broadcasting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparisons-masks-and-boolean-logic">
     Comparisons, Masks, and Boolean Logic
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fancy-indexing">
     Fancy Indexing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sorting-arrays">
     Sorting Arrays
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-data-numpy-s-structured-arrays">
     Structured Data: NumPy’s Structured Arrays
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pandas-panda-face">
   2. Pandas :panda_face:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introducing-pandas-objects">
     Introducing Pandas Objects
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-indexing-and-selection">
     Data Indexing and Selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#operating-on-data-in-pandas">
     Operating on Data in Pandas
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#handling-missing-data">
     Handling Missing Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-indexing">
     Hierarchical Indexing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-datasets-concat-and-append">
     Combining Datasets: Concat and Append
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-datasets-merge-and-join">
     Combining Datasets: Merge and Join
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aggregation-and-grouping">
     Aggregation and Grouping
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pivotal-tables">
     Pivotal tables
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vectorized-string-operations">
     Vectorized String Operations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-time-series">
     Working with Time Series
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#high-performance-pandas-eval-and-query">
     High-Performance Pandas: eval() and query()
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#further-resources">
     Further Resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-computer">
   3. Machine Learning :computer:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-machine-learning">
     What Is Machine Learning?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introducing-scikit-learn">
     Introducing Scikit-Learn
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters-and-model-validation">
     Hyperparameters and Model Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-engineering">
     Feature Engineering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-naive-bayes-classification">
     In Depth: Naive Bayes Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-linear-regression">
     In Depth: Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-support-vector-machines">
     In-Depth: Support Vector Machines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-decision-trees-and-random-forests">
     In-Depth: Decision Trees and Random Forests
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-principal-component-analysis">
     In Depth: Principal Component Analysis
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-manifold-learning">
     In-Depth: Manifold Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-k-means-clustering">
     In Depth: k-Means Clustering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#in-depth-gaussian-mixture-models">
     In Depth: Gaussian Mixture Models
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="book-python-data-science-handbook">
<h1>[book] Python Data Science Handbook<a class="headerlink" href="#book-python-data-science-handbook" title="Permalink to this headline">¶</a></h1>
<hr>
* A study note of {cite:p}`vanderplas2016python` *
<div class="section" id="numpy">
<h2>1. Numpy<a class="headerlink" href="#numpy" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-types-in-python-snake">
<h3>Data types in Python :snake:<a class="headerlink" href="#data-types-in-python-snake" title="Permalink to this headline">¶</a></h3>
<p><img alt="输入图片描述" src="https://raw.githubusercontent.com/askming/picgo/master/69d7b1a7_20200421112255.png" /></p>
<p><img alt="输入图片描述" src="https://raw.githubusercontent.com/askming/picgo/master/01b18a2e_20200421112320.png" /></p>
<ul class="simple">
<li><p>Fixed-type NumPy-style arrays lack this flexibility, but are much more efficient for storing and manipulating data.</p></li>
<li><p>The standard Python implementation is written in C. This means that every Python object is simply a cleverly-disguised C structure, which contains not only its value, but other information as well.</p></li>
<li><p>A Python integer is a pointer to a position in memory containing all the Python object information, including the bytes that contain the integer value. This extra information in the Python integer structure is what allows Python to be coded so freely and dynamically. All this additional information in Python types comes at a cost, however, which becomes especially apparent in structures that combine many of these objects.</p></li>
<li><p>Much more useful, however, is the <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> object of the NumPy package. While Python’s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data.</p></li>
</ul>
</div>
<div class="section" id="basics-of-numpy-arrays">
<h3>Basics of NumPy arrays<a class="headerlink" href="#basics-of-numpy-arrays" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>np arrary attributes: dimension, shape, size, dtype, itemsize, etc.</p></li>
<li><p>Array indexing: Accessing Single Elements</p></li>
<li><p>Array Slicing: Accessing Subarrays</p>
<ul>
<li><p>one-dimensional array</p></li>
<li><p>multi-dimensional</p></li>
</ul>
</li>
<li><p>Reshaping of Arrays</p></li>
<li><p>Array Concatenation and Splitting</p></li>
</ul>
</details>
<ul class="simple">
<li><p>The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x, use this:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">stop</span><span class="p">:</span><span class="n">step</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If any of these are unspecified, they default to the values start=0, stop=size of dimension, step=1.</p></li>
<li><p>Other attributes include <code class="docutils literal notranslate"><span class="pre">itemsize</span></code>, which lists the size (in bytes) of each array element, and <code class="docutils literal notranslate"><span class="pre">nbytes</span></code>, which lists the total size (in bytes) of the array</p></li>
<li><p>In a multi-dimensional array, items can be accessed using a comma-separated tuple of indices:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
       
<span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">3</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In slicing, a potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array. (便捷地反转一个array的方法)</p></li>
<li><p>Keep in mind that, unlike Python lists, <strong>NumPy arrays have a fixed type</strong>. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated.</p></li>
<li><p>One important–and extremely useful–thing to know about array slices is that they return <em>views</em> rather than <em>copies</em> of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies.</p></li>
<li><p>If we modify the subarray, we’ll see that the original array is changed! This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.</p></li>
<li><p>In the case of row access, the empty slice can be omitted for a more compact syntax:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># equivalent to x2[0, :]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. This can be done with the reshape method, or more easily done by making use of the newaxis keyword within a slice operation:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># row vector via reshape</span>
<span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># row vector via newaxis</span>
<span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="section" id="computation-on-numpy-arrays-universal-functions">
<h3>Computation on NumPy Arrays: Universal Functions<a class="headerlink" href="#computation-on-numpy-arrays-universal-functions" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>The Slowness of Loops</p></li>
<li><p>Introducing UFuncs</p></li>
<li><p>Exploring NumPy’s UFuncs</p>
<ul>
<li><p>Array arithmetic</p></li>
<li><p>Absolute value</p></li>
<li><p>Trigonometric functions</p></li>
<li><p>Exponents and logarithms</p></li>
<li><p>Specialized ufuncs</p></li>
</ul>
</li>
<li><p>Advanced Ufunc Features</p>
<ul>
<li><p>Specifying output</p></li>
<li><p>Aggregates</p></li>
<li><p>Outer products</p></li>
</ul>
</li>
<li><p>Ufuncs: Learning More</p></li>
</ul>
</details>
<ul class="simple">
<li><p>Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible – before we saw an operation between a scalar and an array, but we can also operate between two arrays</p></li>
<li><p>Ufuncs exist in two flavors: <em>unary ufuncs</em>, which operate on a single input, and <em>binary ufuncs</em>, which operate on two inputs.</p></li>
<li><p>Each of these arithmetic operations are simply convenient wrappers around specific functions built into NumPy</p></li>
<li><p>The following table lists the arithmetic operators implemented in NumPy:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Operator</p></th>
<th class="head"><p>Equivalent ufunc</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">+</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.add</span></code></p></td>
<td><p>Addition (e.g., <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">2</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.subtract</span></code></p></td>
<td><p>Subtraction (e.g., <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">-</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">1</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.negative</span></code></p></td>
<td><p>Unary negation (e.g., <code class="docutils literal notranslate"><span class="pre">-2</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">*</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.multiply</span></code></p></td>
<td><p>Multiplication (e.g., <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">3</span> <span class="pre">=</span> <span class="pre">6</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">/</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.divide</span></code></p></td>
<td><p>Division (e.g., <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">/</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">1.5</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">//</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.floor_divide</span></code></p></td>
<td><p>Floor division (e.g., <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">=</span> <span class="pre">1</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">**</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.power</span></code></p></td>
<td><p>Exponentiation (e.g., <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">3</span> <span class="pre">=</span> <span class="pre">8</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">%</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.mod</span></code></p></td>
<td><p>Modulus/remainder (e.g., <code class="docutils literal notranslate"><span class="pre">9</span> <span class="pre">%</span> <span class="pre">4</span> <span class="pre">=</span> <span class="pre">1</span></code>)</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>There are also some specialized versions that are useful for maintaining precision with very small input:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;exp(x) - 1 =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;log(1 + x) =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.</span>          <span class="mf">0.0010005</span>   <span class="mf">0.01005017</span>  <span class="mf">0.10517092</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.</span>          <span class="mf">0.0009995</span>   <span class="mf">0.00995033</span>  <span class="mf">0.09531018</span><span class="p">]</span>
</pre></div>
</div>
<p>When x is very small, these functions give more precise values than if the raw <code class="docutils literal notranslate"><span class="pre">np.log</span> </code>or <code class="docutils literal notranslate"><span class="pre">np.exp</span></code> were to be used.</p>
<ul class="simple">
<li><p>Specifying the output:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">y</span><span class="p">[::</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> 

<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span>  <span class="mf">1.</span>   <span class="mf">0.</span>   <span class="mf">2.</span>   <span class="mf">0.</span>   <span class="mf">4.</span>   <span class="mf">0.</span>   <span class="mf">8.</span>   <span class="mf">0.</span>  <span class="mf">16.</span>   <span class="mf">0.</span><span class="p">]</span>
</pre></div>
</div>
<p>If we had instead written <code class="docutils literal notranslate"><span class="pre">y[::2]</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">x</span></code>, this would have resulted in the creation of a temporary array to hold the results of <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">x</span></code>, followed by a second operation copying those values into the y array. This doesn’t make much of a difference for such a small computation, but for very large arrays the memory savings from careful use of the out argument can be significant.</p>
<ul class="simple">
<li><p>Another excellent source for more specialized and obscure ufuncs is the submodule <code class="docutils literal notranslate"><span class="pre">scipy.special</span></code>. If you want to compute some obscure mathematical function on your data, chances are it is implemented in <code class="docutils literal notranslate"><span class="pre">scipy.special</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">ufunc.at</span></code> and <code class="docutils literal notranslate"><span class="pre">ufunc.reduceat</span></code> methods, which we’ll explore in Fancy Indexing, are very helpful as well.</p></li>
<li><p>More information on universal functions (including the full list of available functions) can be found on the <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> and <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> documentation websites.</p></li>
</ul>
</div>
<div class="section" id="aggregations-min-max-and-everything-in-between">
<h3>Aggregations: Min, Max, and Everything In Between<a class="headerlink" href="#aggregations-min-max-and-everything-in-between" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Summing the Values in an Array</p></li>
<li><p>Minimum and Maximum</p>
<ul>
<li><p>Multi dimensional aggregates</p></li>
<li><p>Other aggregation functions</p></li>
</ul>
</li>
</ul>
</details>
<ul class="simple">
<li><p>For min, max, sum, and several other NumPy aggregates, a shorter syntax is to use methods of the array object itself:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">big_array</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">big_array</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">big_array</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">axis</span></code> keyword specifies <strong>the dimension of the array that will be collapsed, rather than the dimension that will be returned</strong>. So specifying <code class="docutils literal notranslate"><span class="pre">axis=0</span></code> means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated.</p></li>
<li><p>Most aggregates have a <code class="docutils literal notranslate"><span class="pre">NaN</span></code>-safe counterpart that computes the result while ignoring missing values, which are marked by the special IEEE floating-point NaN value (for a fuller discussion of missing data, see Handling Missing Data).The following table provides a list of useful aggregation functions available in <code class="docutils literal notranslate"><span class="pre">NumPy</span></code>:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Function Name</p></th>
<th class="head"><p>NaN-safe Version</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>np.sum</p></td>
<td><p>np.nansum</p></td>
<td><p>Compute sum of elements</p></td>
</tr>
<tr class="row-odd"><td><p>np.prod</p></td>
<td><p>np.nanprod</p></td>
<td><p>Compute product of elements</p></td>
</tr>
<tr class="row-even"><td><p>np.mean</p></td>
<td><p>np.nanmean</p></td>
<td><p>Compute mean of elements</p></td>
</tr>
<tr class="row-odd"><td><p>np.std</p></td>
<td><p>np.nanstd</p></td>
<td><p>Compute standard deviation</p></td>
</tr>
<tr class="row-even"><td><p>np.var</p></td>
<td><p>np.nanvar</p></td>
<td><p>Compute variance</p></td>
</tr>
<tr class="row-odd"><td><p>np.min</p></td>
<td><p>np.nanmin</p></td>
<td><p>Find minimum value</p></td>
</tr>
<tr class="row-even"><td><p>np.max</p></td>
<td><p>np.nanmax</p></td>
<td><p>Find maximum value</p></td>
</tr>
<tr class="row-odd"><td><p>np.argmin</p></td>
<td><p>np.nanargmin</p></td>
<td><p>Find index of minimum value</p></td>
</tr>
<tr class="row-even"><td><p>np.argmax</p></td>
<td><p>np.nanargmax</p></td>
<td><p>Find index of maximum value</p></td>
</tr>
<tr class="row-odd"><td><p>np.median</p></td>
<td><p>np.nanmedian</p></td>
<td><p>Compute median of elements</p></td>
</tr>
<tr class="row-even"><td><p>np.percentile</p></td>
<td><p>np.nanpercentile</p></td>
<td><p>Compute rank-based statistics of elements</p></td>
</tr>
<tr class="row-odd"><td><p>np.any</p></td>
<td><p>N/A</p></td>
<td><p>Evaluate whether any elements are true</p></td>
</tr>
<tr class="row-even"><td><p>np.all</p></td>
<td><p>N/A</p></td>
<td><p>Evaluate whether all elements are true</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="computation-on-arrays-broadcasting">
<h3>Computation on Arrays: Broadcasting<a class="headerlink" href="#computation-on-arrays-broadcasting" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Introducing Broadcasting</p></li>
<li><p>Rules of Broadcasting</p>
<ul>
<li><p>Broadcasting examples 1/2/3</p></li>
</ul>
</li>
<li><p>Broadcasting in Practice</p>
<ul>
<li><p>Centering an array</p></li>
<li><p>Plotting a two-dimensional function</p></li>
</ul>
</li>
</ul>
</details>
<ul class="simple">
<li><p>Broadcasting is simply a set of rules for applying binary ufuncs (e.g., addition, subtraction, multiplication, etc.) on arrays of different sizes.</p></li>
<li><p>For arrays of the same size, binary operations are performed on an element-by-element basis</p></li>
<li><p>The advantage of <code class="docutils literal notranslate"><span class="pre">NumPy</span></code>’s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting.</p></li>
<li><p>Broadcasting in <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> follows a strict set of rules to determine the interaction between the two arrays:</p>
<ul>
<li><p>Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.</p></li>
<li><p>Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.</p></li>
<li><p>Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised.
<img alt="输入图片描述" src="https://raw.githubusercontent.com/askming/picgo/master/02.05-broadcasting_20200608033656.png" /></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="comparisons-masks-and-boolean-logic">
<h3>Comparisons, Masks, and Boolean Logic<a class="headerlink" href="#comparisons-masks-and-boolean-logic" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Example</p></li>
<li><p>Comparison Operators as ufuncs</p></li>
<li><p>Working with Boolean Arrays</p>
<ul>
<li><p>Counting entries</p></li>
<li><p>Boolean operators</p></li>
</ul>
</li>
<li><p>Boolean Arrays as Masks</p></li>
<li><p>Aside: Using the Keywords <code class="docutils literal notranslate"><span class="pre">and/or</span></code> Versus the Operators <code class="docutils literal notranslate"><span class="pre">&amp;/|</span></code></p></li>
</ul>
</details>
<ul class="simple">
<li><p>A quick warning: as mentioned in Aggregations: Min, Max, and Everything In Between, Python has built-in <code class="docutils literal notranslate"><span class="pre">sum()</span></code>, <code class="docutils literal notranslate"><span class="pre">any()</span></code>, and <code class="docutils literal notranslate"><span class="pre">all()</span></code> functions. These have a different syntax than the <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> versions, and in particular will fail or produce unintended results when used on multidimensional arrays. Be sure that you are using <code class="docutils literal notranslate"><span class="pre">np.sum()</span></code>, <code class="docutils literal notranslate"><span class="pre">np.any()</span></code>, and <code class="docutils literal notranslate"><span class="pre">np.all()</span></code> for these examples!</p></li>
<li><p>When you use <code class="docutils literal notranslate"><span class="pre">and</span></code> or <code class="docutils literal notranslate"><span class="pre">or</span></code>, it’s equivalent to asking Python to treat the object as a single Boolean entity. So remember this: <code class="docutils literal notranslate"><span class="pre">and</span></code> and <code class="docutils literal notranslate"><span class="pre">or</span></code> perform a single Boolean evaluation on an entire object, while <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">|</span></code> perform multiple Boolean evaluations on the content (the individual bits or bytes) of an object. For Boolean <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> arrays, the latter is nearly always the desired operation.</p></li>
</ul>
</div>
<div class="section" id="fancy-indexing">
<h3>Fancy Indexing<a class="headerlink" href="#fancy-indexing" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Exploring Fancy Indexing</p></li>
<li><p>Combined Indexing</p></li>
<li><p>Example: Selecting Random Points</p></li>
<li><p>Modifying Values with Fancy Indexing</p></li>
<li><p>Example: Binning Data</p></li>
</ul>
</details>
<ul class="simple">
<li><p>Fancy indexing is like the simple indexing we’ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array’s values.</p></li>
<li><p>When using fancy indexing, <strong>the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed</strong>:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[</span><span class="mi">71</span><span class="p">,</span> <span class="mi">86</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">20</span><span class="p">]])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It is always important to remember with fancy indexing that the return value reflects the broadcasted shape of the indices, rather than the shape of the array being indexed.</p></li>
<li><p>One common use of fancy indexing is the selection of subsets of rows from a matrix.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">20</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">selection</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>  <span class="c1"># fancy indexing here</span>
<span class="n">selection</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span> <span class="mf">6.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
<span class="n">i</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">x</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([</span> <span class="mf">6.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x[i]</span> <span class="pre">+=</span> <span class="pre">1</span></code> is meant as a shorthand of <code class="docutils literal notranslate"><span class="pre">x[i]</span> <span class="pre">=</span> <span class="pre">x[i]</span> <span class="pre">+</span> <span class="pre">1</span></code>. <code class="docutils literal notranslate"><span class="pre">x[i]</span> <span class="pre">+</span> <span class="pre">1</span></code> is evaluated, and then the result is assigned to the indices in <code class="docutils literal notranslate"><span class="pre">x</span></code>. With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results.</p></li>
<li><p>What if you want the other behavior where the operation is repeated? For this, you can use the <code class="docutils literal notranslate"><span class="pre">at()</span></code> method of ufuncs (available since NumPy 1.8), and do the following:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">1.</span>  <span class="mf">2.</span>  <span class="mf">3.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The pairing of indices in fancy indexing follows all the broadcasting rules that were mentioned in Computation on Arrays: Broadcasting.</p></li>
</ul>
</div>
<div class="section" id="sorting-arrays">
<h3>Sorting Arrays<a class="headerlink" href="#sorting-arrays" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Fast Sorting in <code class="docutils literal notranslate"><span class="pre">NumPy</span></code>: <code class="docutils literal notranslate"><span class="pre">np.sort</span></code> and <code class="docutils literal notranslate"><span class="pre">np.argsort</span></code></p>
<ul>
<li><p>Sorting along rows or columns</p></li>
</ul>
</li>
<li><p>Partial Sorts: Partitioning</p></li>
<li><p>Example: k-Nearest Neighbors</p></li>
<li><p>Aside: Big-O Notation</p></li>
</ul>
</details>
<ul class="simple">
<li><p>As any first-year computer science major will tell you, the <strong>selection sort</strong> is useful for its simplicity, but is much too slow to be useful for larger arrays. For a list of  <span class="math notranslate nohighlight">\(N\)</span>  values, it requires <span class="math notranslate nohighlight">\(N\)</span>  loops, each of which does on order <span class="math notranslate nohighlight">\(\sim N\)</span>  comparisons to find the swap value. In terms of the “big-O” notation often used to characterize these algorithms (see Big-O Notation), selection sort averages  <span class="math notranslate nohighlight">\(\mathcal{O}[N^2]\)</span>: if you double the number of items in the list, the execution time will go up by about a factor of four.</p></li>
<li><p>By default <code class="docutils literal notranslate"><span class="pre">np.sort</span></code> uses an <span class="math notranslate nohighlight">\(\mathcal{O}[N \mathcal{log}N]\)</span> , <em>quicksort</em> algorithm, though <em>mergesort</em> and <code class="docutils literal notranslate"><span class="pre">heapsort</span></code> are also available. For most applications, the default quicksort is more than sufficient.</p></li>
<li><p>To return a sorted version of the array without modifying the input, you can use <code class="docutils literal notranslate"><span class="pre">np.sort</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A related function is <code class="docutils literal notranslate"><span class="pre">argsort</span></code>, which instead returns the indices of the sorted elements:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">3</span> <span class="mi">2</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If you prefer to sort the array in-place, you can instead use the sort method of arrays:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Sometimes we’re not interested in sorting the entire array, but simply want to find the <em>k</em> smallest values in the array. <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> provides this in the <code class="docutils literal notranslate"><span class="pre">np.partition</span></code> function. <code class="docutils literal notranslate"><span class="pre">np.partition</span></code> takes an array and a number <em>K</em>; the result is a new array with the smallest <em>K</em> values to the left of the partition, and the remaining values to the right, in arbitrary order.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">partition</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Big-O notation, in this loose sense, tells you how much time your algorithm will take as you increase the amount of data. If you have an  <span class="math notranslate nohighlight">\(\mathcal{O}[N]\)</span>  (read “order <span class="math notranslate nohighlight">\(N\)</span>”) algorithm that takes 1 second to operate on a list of length <em>N=1,000</em>, then you should expect it to take roughly 5 seconds for a list of length <em>N=5,000</em>. If you have an  <span class="math notranslate nohighlight">\(\mathcal{O}[N^2]\)</span>  (read “order N squared”) algorithm that takes 1 second for <em>N=1000</em>, then you should expect it to take about 25 seconds for <em>N=5000</em>.</p></li>
<li><p>Notice that the big-O notation by itself tells you nothing about the actual wall-clock time of a computation, but only about its scaling as you change <em>N</em>. Generally, for example, an  <span class="math notranslate nohighlight">\(\mathcal{O}[N]\)</span>  algorithm is considered to have better scaling than an  <span class="math notranslate nohighlight">\(\mathcal{O}[N^2]\)</span>  algorithm, and for good reason. But for small datasets in particular, the algorithm with better scaling might not be faster.</p></li>
</ul>
</div>
<div class="section" id="structured-data-numpy-s-structured-arrays">
<h3>Structured Data: NumPy’s Structured Arrays<a class="headerlink" href="#structured-data-numpy-s-structured-arrays" title="Permalink to this headline">¶</a></h3>
<p><em>less important - use more pandas instead of numpy structed array</em></p>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Creating Structured Arrays</p></li>
<li><p>More Advanced Compound Types</p></li>
<li><p>RecordArrays: Structured Arrays with a Twist</p></li>
<li><p>On to Pandas</p></li>
</ul>
</details>
<ul class="simple">
<li><p>The handy thing with structured arrays is that you can now refer to values either by index or by name</p></li>
<li><p>If the names of the types do not matter to you, you can specify the types alone in a comma-separated string</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;S10,i4,f8&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dtype</span><span class="p">([(</span><span class="s1">&#39;f0&#39;</span><span class="p">,</span> <span class="s1">&#39;S10&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;i4&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;f2&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;f8&#39;</span><span class="p">)])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The shortened string format codes may seem confusing, but they are built on simple principles. The first (optional) character is <code class="docutils literal notranslate"><span class="pre">&lt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&gt;</span></code>, which means “little endian” or “big endian,” respectively, and specifies the ordering convention for significant bits. The next character specifies the type of data: characters, bytes, ints, floating points, and so on (see the table below). The last character or characters represents the size of the object in bytes.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Character</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>‘b’</p></td>
<td><p>Byte</p></td>
<td><p>np.dtype(‘b’)</p></td>
</tr>
<tr class="row-odd"><td><p>‘i’</p></td>
<td><p>Signed integer</p></td>
<td><p>np.dtype(‘i4’) == np.int32</p></td>
</tr>
<tr class="row-even"><td><p>‘u’</p></td>
<td><p>Unsigned integer</p></td>
<td><p>np.dtype(‘u1’) == np.uint8</p></td>
</tr>
<tr class="row-odd"><td><p>‘f’</p></td>
<td><p>Floating point</p></td>
<td><p>np.dtype(‘f8’) == np.int64</p></td>
</tr>
<tr class="row-even"><td><p>‘c’</p></td>
<td><p>Complex floating point</p></td>
<td><p>np.dtype(‘c16’) == np.complex128</p></td>
</tr>
<tr class="row-odd"><td><p>‘S’, ‘a’</p></td>
<td><p>String</p></td>
<td><p>np.dtype(‘S5’)</p></td>
</tr>
<tr class="row-even"><td><p>‘U’</p></td>
<td><p>Unicode string</p></td>
<td><p>np.dtype(‘U’) == np.str_</p></td>
</tr>
<tr class="row-odd"><td><p>‘V’</p></td>
<td><p>Raw data (void)</p></td>
<td><p>np.dtype(‘V’) == np.void</p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
</div>
</div>
<div class="section" id="pandas-panda-face">
<h2>2. Pandas :panda_face:<a class="headerlink" href="#pandas-panda-face" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introducing-pandas-objects">
<h3>Introducing Pandas Objects<a class="headerlink" href="#introducing-pandas-objects" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul class="simple">
<li><p>The Pandas Series Object</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Series</span></code> as generalized NumPy array</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Series</span></code> as specialized dictionary</p></li>
<li><p>Constructing Series objects</p></li>
</ul>
</li>
<li><p>The Pandas DataFrame Object</p>
<ul>
<li><p>DataFrame as a generalized NumPy array</p></li>
<li><p>DataFrame as specialized dictionary</p></li>
<li><p>Constructing DataFrame objects</p>
<ul>
<li><p>From a single Series object</p></li>
<li><p>From a list of dicts</p></li>
<li><p>From a dictionary of Series objects</p></li>
<li><p>From a two-dimensional NumPy array</p></li>
<li><p>From a NumPy structured array</p></li>
</ul>
</li>
</ul>
</li>
<li><p>The Pandas Index Object</p>
<ul>
<li><p>Index as immutable array</p></li>
<li><p>Index as ordered set</p></li>
</ul>
</li>
</ul>
</details>
<ul class="simple">
<li><p>A Pandas Series is a one-dimensional array of indexed data.</p></li>
<li><p>At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. (<em>series as generalized Numpy array</em>)</p></li>
<li><p>You can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure which maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. (<em>series as specilized dictionary</em>)</p></li>
<li><p>Unlike a dictionary, though, the Series also supports array-style operations such as slicing (using its index)</p></li>
<li><p>It may look like the Series object is basically interchangeable with a one-dimensional NumPy array. <strong>The essential difference is the presence of the index</strong>: while the Numpy Array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.</p></li>
<li><p>As we see in the output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. The values are simply a familiar NumPy array</p></li>
<li><p>The pandas Series <code class="docutils literal notranslate"><span class="pre">index</span></code> is an array-like object of type <code class="docutils literal notranslate"><span class="pre">pd.Index</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">index</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">RangeIndex</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="data-indexing-and-selection">
<h3>Data Indexing and Selection<a class="headerlink" href="#data-indexing-and-selection" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Data Selection in Series</p>
<ul>
<li><p>Series as dictionary</p></li>
<li><p>Series as one-dimensional array</p></li>
<li><p>Indexers: loc, iloc, and ix</p></li>
</ul>
</li>
<li><p>Data Selection in DataFrame</p>
<ul>
<li><p>DataFrame as a dictionary</p></li>
<li><p>DataFrame as two-dimensional array</p></li>
<li><p>Additional indexing conventions</p></li>
</ul>
</li>
</ul>
</details>
<ul class="simple">
<li><p>There are a couple extra indexing conventions that might seem at odds with the preceding discussion, but nevertheless can be very useful in practice. First, while <em>indexing</em> refers to columns, <em>slicing</em> refers to rows</p></li>
<li><p>Pandas provides some special indexer attributes, these are not functional methods, but attributes that expose a particular slicing interface to the data in the Series.</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">loc</span></code> attribute allows indexing and slicing that always references the <strong>explicit index</strong>, this means explicit index and column names in DataFrames.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">iloc</span></code> attribute allows indexing and slicing that always references the <strong>implicit Python-style index</strong></p></li>
<li><p>A third indexing attribute, ix, is a hybrid of the two, and for Series objects is equivalent to standard []-based indexing. The purpose of the ix indexer will become more apparent in the context of DataFrame objects, where both implicit index or expilcit index and column name can be used.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">data</span>

<span class="o">&gt;&gt;&gt;</span> 
<span class="mi">1</span>    <span class="n">a</span>
<span class="mi">3</span>    <span class="n">b</span>
<span class="mi">5</span>    <span class="n">c</span>
<span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>

<span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="s1">&#39;a&#39;</span>

<span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="s1">&#39;b&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>One guiding principle of Python code is that “explicit is better than implicit.” The explicit nature of <code class="docutils literal notranslate"><span class="pre">loc</span></code> and <code class="docutils literal notranslate"><span class="pre">iloc</span></code> make them very useful in maintaining clean and readable code</p></li>
<li><p>a DataFrame acts in many ways like a two-dimensional or structured array, and in other ways like a dictionary of Series structures sharing the same index. These analogies can be helpful to keep in mind as we explore data selection within this structure.</p></li>
<li><p>we can use attribute-style access with column names that are strings</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">area</span>
<span class="o">&gt;&gt;&gt;</span> 
<span class="n">California</span>    <span class="mi">423967</span>
<span class="n">Florida</span>       <span class="mi">170312</span>
<span class="n">Illinois</span>      <span class="mi">149995</span>
<span class="n">New</span> <span class="n">York</span>      <span class="mi">141297</span>
<span class="n">Texas</span>         <span class="mi">695662</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">area</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Avoid the temptation to try column assignment via attribute (i.e., use data[‘pop’] = z rather than data.pop = z)</p></li>
</ul>
</div>
<div class="section" id="operating-on-data-in-pandas">
<h3>Operating on Data in Pandas<a class="headerlink" href="#operating-on-data-in-pandas" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Ufuncs: Index Preservation</p></li>
<li><p>UFuncs: Index Alignment</p>
<ul>
<li><p>Index alignment in Series</p></li>
<li><p>Index alignment in DataFrame</p></li>
</ul>
</li>
<li><p>Ufuncs: Operations Between DataFrame and Series</p></li>
</ul>
</details>
<ul class="simple">
<li><p>Pandas includes a couple useful twists, however: for unary operations like negation and trigonometric functions, these ufuncs will preserve index and column labels in the output, and for binary operations such as addition and multiplication, Pandas will automatically <em>align indices</em> when passing the objects to the ufunc.</p></li>
<li><p>For binary operations on two Series or DataFrame objects, Pandas will align indices in the process of performing the operation. This is very convenient when working with incomplete data</p></li>
<li><p>Any item for which one or the other does not have an entry is marked with <code class="docutils literal notranslate"><span class="pre">NaN</span></code>, or “Not a Number,” which is how Pandas marks missing data (see further discussion of missing data in Handling Missing Data). This index matching is implemented this way for any of Python’s built-in arithmetic expressions; any missing values are filled in with <code class="docutils literal notranslate"><span class="pre">NaN</span></code> by default</p></li>
<li><p>If using <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values is not the desired behavior, the fill value can be modified using appropriate object methods in place of the operators.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="mi">0</span>    <span class="mf">2.0</span>
<span class="mi">1</span>    <span class="mf">5.0</span>
<span class="mi">2</span>    <span class="mf">9.0</span>
<span class="mi">3</span>    <span class="mf">5.0</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This preservation and alignment of indices and columns means that operations on data in Pandas will always maintain the data context, which prevents the types of silly errors that might come up when working with heterogeneous and/or misaligned data in raw NumPy arrays.</p></li>
</ul>
</div>
<div class="section" id="handling-missing-data">
<h3>Handling Missing Data<a class="headerlink" href="#handling-missing-data" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Trade-Offs in Missing Data Conventions</p></li>
<li><p>Missing Data in Pandas</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: Pythonic missing data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NaN</span></code>: Missing numerical data</p></li>
<li><p>NaN and None in Pandas</p></li>
</ul>
</li>
<li><p>Operating on Null Values</p>
<ul>
<li><p>Detecting null values</p></li>
<li><p>Dropping null values</p></li>
<li><p>Filling null values</p></li>
</ul>
</li>
</ul>
</details>
- There are a number of schemes that have been developed to indicate the presence of missing data in a table or DataFrame. Generally, they revolve around one of two strategies: using a mask that globally indicates missing values, or choosing a sentinel value that indicates a missing entry. None of these approaches is without trade-offs: use of a separate mask array requires allocation of an additional Boolean array, which adds overhead in both storage and computation. A sentinel value reduces the range of valid values that can be represented, and may require extra (often non-optimized) logic in CPU and GPU arithmetic. Common special values like NaN are not available for all data types.
<ul class="simple">
<li><p>With these constraints in mind, <strong>Pandas chose to use sentinels for missing data</strong>, and further chose to use two already-existing Python <code class="docutils literal notranslate"><span class="pre">null</span></code> values: the special floating-point <code class="docutils literal notranslate"><span class="pre">NaN</span></code> value, and the Python None object. This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest.</p></li>
<li><p>The way in which Pandas handles missing values is constrained by its reliance on the NumPy package, which does not have a built-in notion of <code class="docutils literal notranslate"><span class="pre">NA</span></code> values for non-floating-point data types.</p></li>
<li><p>Because it is a Python object, <code class="docutils literal notranslate"><span class="pre">None</span></code> cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type ‘object’ (i.e., arrays of Python objects)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vals1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">vals1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">dtype=object</span></code> means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects.  While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for arrays with native types.</p>
<ul class="simple">
<li><p>The use of Python objects in an array also means that if you perform aggregations like <code class="docutils literal notranslate"><span class="pre">sum()</span></code> or <code class="docutils literal notranslate"><span class="pre">min()</span></code> across an array with a <code class="docutils literal notranslate"><span class="pre">None</span></code> value, you will generally get an error</p></li>
<li><p>The following table lists the upcasting conventions in Pandas when NA values are introduced:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Typeclass</p></th>
<th class="head"><p>Conversion When Storing NAs</p></th>
<th class="head"><p>NA Sentinel Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">floating</span></code></p></td>
<td><p>No change</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.nan</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">object</span></code></p></td>
<td><p>No change</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">np.nan</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">integer</span></code></p></td>
<td><p>Cast to <code class="docutils literal notranslate"><span class="pre">float64</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.nan</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">boolean</span></code></p></td>
<td><p>Cast to object</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">np.nan</span></code></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Keep in mind that in Pandas, string data is always stored with an <code class="docutils literal notranslate"><span class="pre">object</span></code> dtype.</p></li>
<li><p>The other missing data representation, <code class="docutils literal notranslate"><span class="pre">NaN</span></code> (acronym for Not a Number), is different; it is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vals2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span> 
<span class="n">vals2</span><span class="o">.</span><span class="n">dtype</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">dtype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code. You should be aware that <code class="docutils literal notranslate"><span class="pre">NaN</span></code> is a bit like a data virus–it infects any other object it touches.</p></li>
<li><p>Keep in mind that <code class="docutils literal notranslate"><span class="pre">NaN</span></code> is specifically a floating-point value; there is no equivalent <code class="docutils literal notranslate"><span class="pre">NaN</span></code> value for integers, strings, or other types.</p></li>
<li><p>Pandas treats <code class="docutils literal notranslate"><span class="pre">None</span></code> and <code class="docutils literal notranslate"><span class="pre">NaN</span></code> as essentially interchangeable for indicating missing or null values. To facilitate this convention, there are several useful methods for detecting, removing, and replacing <code class="docutils literal notranslate"><span class="pre">null</span></code> values in Pandas data structures.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">isnull()</span></code>: Generate a boolean mask indicating missing values</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">notnull()</span></code>: Opposite of <code class="docutils literal notranslate"><span class="pre">isnull()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropna()</span></code>: Return a filtered version of the data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fillna()</span></code>: Return a copy of the data with missing values filled or imputed</p></li>
</ul>
</li>
<li><ul>
<li><p>In addition to the masking used before, there are the convenience methods, dropna() (which removes NA values) and fillna() (which fills in NA values)</p></li>
</ul>
</li>
<li><p>By default, dropna() will drop all rows in which any null value is present; you might rather be interested in dropping rows or columns with all <code class="docutils literal notranslate"><span class="pre">NA</span></code> values, or a majority of <code class="docutils literal notranslate"><span class="pre">NA</span></code> values. This can be specified through the how or thresh parameters, which allow fine control of the number of nulls to allow through.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;rows&#39;</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="hierarchical-indexing">
<h3>Hierarchical Indexing<a class="headerlink" href="#hierarchical-indexing" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>A Multiply Indexed Series</p>
<ul>
<li><p>The bad way</p></li>
<li><p>The Better Way: Pandas MultiIndex</p></li>
<li><p>MultiIndex as extra dimension</p></li>
</ul>
</li>
<li><p>Methods of MultiIndex Creation</p>
<ul>
<li><p>Explicit MultiIndex constructors</p></li>
<li><p>MultiIndex level names</p></li>
<li><p>MultiIndex for columns</p></li>
</ul>
</li>
<li><p>Indexing and Slicing a MultiIndex</p>
<ul>
<li><p>Multiply indexed Series</p></li>
<li><p>Multiply indexed DataFrames</p></li>
</ul>
</li>
<li><p>Rearranging Multi-Indices</p>
<ul>
<li><p>Sorted and unsorted indices</p></li>
<li><p>Stacking and unstacking indices</p></li>
<li><p>Index setting and resetting</p></li>
</ul>
</li>
<li><p>Data Aggregations on Multi-Indices</p></li>
<li><p>Aside: Panel Data</p></li>
</ul>
</details>
<ul class="simple">
<li><p>While Pandas does provide <code class="docutils literal notranslate"><span class="pre">Panel</span></code> and <code class="docutils literal notranslate"><span class="pre">Panel4D</span></code> objects that natively handle three-dimensional and four-dimensional data (see Aside: Panel Data), a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects.</p></li>
<li><p>The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor.</p></li>
<li><p>Pandas is built with this equivalence in mind. The unstack() method will quickly convert a multiply indexed Series into a conventionally indexed DataFrame and naturally, the stack() method provides the opposite operation</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pop_df</span> <span class="o">=</span> <span class="n">pop</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>
<span class="n">pop</span> <span class="o">=</span> <span class="n">pop_df</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Seeing this, you might wonder why would we would bother with hierarchical indexing at all. The reason is simple: just as we were able to use multi-indexing to represent two-dimensional data within a one-dimensional Series, we can also use it to represent data of three or more dimensions in a Series or DataFrame. Each extra level in a multi-index represents an extra dimension of data; taking advantage of this property gives us much more flexibility in the types of data we can represent.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                  <span class="n">index</span><span class="o">=</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
                  <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data1&#39;</span><span class="p">,</span> <span class="s1">&#39;data2&#39;</span><span class="p">])</span>
<span class="n">df</span>
</pre></div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>data1</p></th>
<th class="head"><p>data2</p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>a</p></td>
<td><p>1</p></td>
<td><p>0.554233</p></td>
<td><p>0.356072</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>0.925244</p></td>
<td><p>0.219474</p></td>
</tr>
<tr class="row-even"><td><p>b</p></td>
<td><p>1</p></td>
<td><p>0.441759</p></td>
<td><p>0.610054</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>0.171495</p></td>
<td><p>0.886688</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>The work of creating the MultiIndex is done in the background. Similarly, if you pass a dictionary with appropriate tuples as keys, Pandas will automatically recognize this and use a <code class="docutils literal notranslate"><span class="pre">MultiIndex</span></code> by default:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">&#39;California&#39;</span><span class="p">,</span> <span class="mi">2000</span><span class="p">):</span> <span class="mi">33871648</span><span class="p">,</span>
        <span class="p">(</span><span class="s1">&#39;California&#39;</span><span class="p">,</span> <span class="mi">2010</span><span class="p">):</span> <span class="mi">37253956</span><span class="p">,</span>
        <span class="p">(</span><span class="s1">&#39;Texas&#39;</span><span class="p">,</span> <span class="mi">2000</span><span class="p">):</span> <span class="mi">20851820</span><span class="p">,</span>
        <span class="p">(</span><span class="s1">&#39;Texas&#39;</span><span class="p">,</span> <span class="mi">2010</span><span class="p">):</span> <span class="mi">25145561</span><span class="p">,</span>
        <span class="p">(</span><span class="s1">&#39;New York&#39;</span><span class="p">,</span> <span class="mi">2000</span><span class="p">):</span> <span class="mi">18976457</span><span class="p">,</span>
        <span class="p">(</span><span class="s1">&#39;New York&#39;</span><span class="p">,</span> <span class="mi">2010</span><span class="p">):</span> <span class="mi">19378102</span><span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span>
<span class="n">California</span>  <span class="mi">2000</span>    <span class="mi">33871648</span>
            <span class="mi">2010</span>    <span class="mi">37253956</span>
<span class="n">New</span> <span class="n">York</span>    <span class="mi">2000</span>    <span class="mi">18976457</span>
            <span class="mi">2010</span>    <span class="mi">19378102</span>
<span class="n">Texas</span>       <span class="mi">2000</span>    <span class="mi">20851820</span>
            <span class="mi">2010</span>    <span class="mi">25145561</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For more flexibility in how the index is constructed, you can instead use the class method constructors available in the <code class="docutils literal notranslate"><span class="pre">pd.MultiIndex</span></code>:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">MultiIndex</span><span class="o">.</span><span class="n">from_arrays</span><span class="p">([[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">MultiIndex</span><span class="o">.</span><span class="n">from_tuples</span><span class="p">([(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For various reasons, partial slices and other similar operations require the levels in the <code class="docutils literal notranslate"><span class="pre">MultiIndex</span></code> to be in sorted (i.e., lexographical) order. Pandas provides a number of convenience routines to perform this type of sorting; examples are the <code class="docutils literal notranslate"><span class="pre">sort_index()</span></code> and <code class="docutils literal notranslate"><span class="pre">sortlevel()</span></code> methods of the DataFrame:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Remember that columns are primary in a DataFrame, and the syntax used for multiply indexed Series applies to the columns.</p></li>
<li><p>Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the <code class="docutils literal notranslate"><span class="pre">reset_index</span></code> method.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pop_flat</span> <span class="o">=</span> <span class="n">pop</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;population&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We’ve previously seen that Pandas has built-in data aggregation methods, such as <code class="docutils literal notranslate"><span class="pre">mean()</span></code>, <code class="docutils literal notranslate"><span class="pre">sum()</span></code>, and <code class="docutils literal notranslate"><span class="pre">max()</span></code>. For hierarchically indexed data, these can be passed a <code class="docutils literal notranslate"><span class="pre">level</span></code> parameter that controls which subset of the data the aggregate is computed on.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data_mean</span> <span class="o">=</span> <span class="n">health_data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="s1">&#39;year&#39;</span><span class="p">)</span> <span class="c1">#average over the year</span>
</pre></div>
</div>
<ul class="simple">
<li><p>By further making use of the axis keyword, we can take the mean among levels on the columns as well:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data_mean</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="c1"># row average over the year </span>
</pre></div>
</div>
<ul class="simple">
<li><p>panel data is fundamentally a dense data representation, while multi-indexing is fundamentally a sparse data representation. As the number of dimensions increases, the dense representation can become very inefficient for the majority of real-world datasets.</p></li>
</ul>
</div>
<div class="section" id="combining-datasets-concat-and-append">
<h3>Combining Datasets: Concat and Append<a class="headerlink" href="#combining-datasets-concat-and-append" title="Permalink to this headline">¶</a></h3>
<details>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Recall: Concatenation of NumPy Arrays</p></li>
<li><p>Simple Concatenation with <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code></p>
<ul>
<li><p>Duplicate indices</p>
<ul>
<li><p>Catching the repeats as an error</p></li>
<li><p>Ignoring the index</p></li>
<li><p>Adding MultiIndex keys</p></li>
</ul>
</li>
<li><p>Concatenation with joins</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">append()</span></code> method</p></li>
</ul>
</li>
</ul>
</details>
<ul class="simple">
<li><p>By default, the concatenation takes place row-wise within the DataFrame (i.e., axis=0/’row’ or <strong>stack</strong>). Like <code class="docutils literal notranslate"><span class="pre">np.concatenate</span></code>, <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> allows specification of an axis along which concatenation will take place.</p></li>
<li><p>One important difference between <code class="docutils literal notranslate"><span class="pre">np.concatenate</span></code> and <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> is that Pandas concatenation preserves indices, even if the result will have duplicate indices! While this is valid within DataFrames, the outcome is often undesirable. pd.concat() gives us a few ways to handle it. (i.e. next bullet point)</p></li>
<li><p>If you’d like to simply verify that the indices in the result of <code class="docutils literal notranslate"><span class="pre">pd.concat()</span></code> do not overlap, you can specify the <code class="docutils literal notranslate"><span class="pre">verify_integrity</span></code> flag. With this set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the concatenation will raise an exception if there are duplicate indices.</p></li>
<li><p>Because direct array concatenation is so common, Series and DataFrame objects have an <code class="docutils literal notranslate"><span class="pre">append</span></code> method that can accomplish the same thing in fewer keystrokes: <code class="docutils literal notranslate"><span class="pre">df1.append(df2)</span></code></p>
<ul>
<li><p>Keep in mind that unlike the <code class="docutils literal notranslate"><span class="pre">append()</span></code> and <code class="docutils literal notranslate"><span class="pre">extend()</span></code> methods of Python lists, the <code class="docutils literal notranslate"><span class="pre">append()</span></code> method in Pandas does not modify the original object–instead it creates a new object with the combined data. Thus, if you plan to do multiple append operations, it is generally better to build a list of DataFrames and pass them all at once to the <code class="docutils literal notranslate"><span class="pre">concat()</span></code> function.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="combining-datasets-merge-and-join">
<h3>Combining Datasets: Merge and Join<a class="headerlink" href="#combining-datasets-merge-and-join" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p><strong>Relational Algebra</strong></p>
<ul class="simple">
<li><p>The behavior implemented in <code class="docutils literal notranslate"><span class="pre">pd.merge()</span></code> is a subset of what is known as <em>relational algebra</em>, which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases.</p></li>
<li><p>The strength of the relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other program, a wide range of fairly complicated composite operations can be performed.</p></li>
</ul>
</li>
<li><p><strong>Categories of Joins</strong></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">pd.merge()</span></code> function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data.</p></li>
<li><p><strong>One-to-one joins</strong></p>
<ul>
<li><p>keep in mind that the merge in general discards the index, except in the special case of merges by index (see the <code class="docutils literal notranslate"><span class="pre">left_index</span></code> and <code class="docutils literal notranslate"><span class="pre">right_index</span></code> keywords)</p></li>
</ul>
</li>
<li><p><strong>Many-to-one joins</strong></p></li>
<li><p><strong>Many-to-many joins</strong></p></li>
</ul>
</li>
<li><p><strong>Specification of the Merge Key</strong></p>
<ul>
<li><p><strong>The <code class="docutils literal notranslate"><span class="pre">on</span></code> keyword</strong></p></li>
<li><p><strong>The <code class="docutils literal notranslate"><span class="pre">left_on</span></code> and <code class="docutils literal notranslate"><span class="pre">right_on</span></code> keywords</strong></p>
<ul class="simple">
<li><p>At times you may wish to merge two datasets with different column names; for example, we may have a dataset in which the employee name is labeled as “name” rather than “employee”. In this case, we can use the <code class="docutils literal notranslate"><span class="pre">left_on</span></code> and <code class="docutils literal notranslate"><span class="pre">right_on</span></code> keywords to specify the two column names</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df3</span><span class="p">,</span> <span class="n">left_on</span><span class="o">=</span><span class="s2">&quot;employee&quot;</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>The <code class="docutils literal notranslate"><span class="pre">left_index</span></code> and <code class="docutils literal notranslate"><span class="pre">right_index</span></code> keywords</strong></p>
<ul class="simple">
<li><p>For convenience, DataFrames implement the <code class="docutils literal notranslate"><span class="pre">join()</span></code> method, which performs a merge that defaults to joining on indices</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df1a</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2a</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If you’d like to mix indices and columns, you can combine <code class="docutils literal notranslate"><span class="pre">left_index</span></code> with <code class="docutils literal notranslate"><span class="pre">right_on</span></code> or <code class="docutils literal notranslate"><span class="pre">left_on</span></code> with <code class="docutils literal notranslate"><span class="pre">right_index</span></code> to get the desired behavior</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df1a</span><span class="p">,</span> <span class="n">df3</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">right_on</span><span class="o">=</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Specifying Set Arithmetic for Joins</strong></p>
<ul class="simple">
<li><p>We can specify this explicitly using the how keyword, which defaults to “inner”</p></li>
<li><p>Other options for the how keyword are ‘outer’, ‘left’, and ‘right’</p></li>
</ul>
</li>
<li><p><strong>Overlapping Column Names: The <code class="docutils literal notranslate"><span class="pre">suffixes</span></code> Keyword</strong></p>
<ul class="simple">
<li><p>In a case where your two input DataFrames have conflicting column names, it is possible to specify a custom suffix using the suffixes keyword</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df8</span><span class="p">,</span> <span class="n">df9</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">suffixes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;_L&quot;</span><span class="p">,</span> <span class="s2">&quot;_R&quot;</span><span class="p">])</span>
</pre></div>
</div>
</li>
</ul>
</details>
</div>
<div class="section" id="aggregation-and-grouping">
<h3>Aggregation and Grouping<a class="headerlink" href="#aggregation-and-grouping" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Planets Data</p></li>
<li><p>Simple Aggregation in Pandas</p>
<ul class="simple">
<li><p>For a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, by default the aggregates (i.e. compute the summary statistics) return results within each column</p></li>
<li><p>There is a convenience method <code class="docutils literal notranslate"><span class="pre">describe()</span></code> that computes several common aggregates for each column and returns the result.</p></li>
</ul>
</li>
<li><p>GroupBy: Split, Apply, Combine
<img alt="输入图片描述" src="https://raw.githubusercontent.com/askming/picgo/master/4762c936_20200421112640.png" /></p></li>
<li><p>This makes clear what the groupby accomplishes:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>- The *split* step involves breaking up and grouping a DataFrame depending on the value of the specified key.
- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.
- The *combine* step merges the results of these operations into an output array.
</pre></div>
</div>
<ul>
<li><p>While this could certainly be done manually using some combination of the masking, aggregation, and merging commands covered earlier, an important realization is that the intermediate splits do not need to be explicitly instantiated. Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole.</p></li>
<li><p>The name “group by” comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine.</p></li>
<li><p>Notice that what is returned is not a set of <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>, but a <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code> object. This object is where the magic is: you can think of it as a special view of the DataFrame, which is poised to dig into the groups but does no actual computation until the aggregation is applied. This “lazy evaluation” approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.</p></li>
<li><p>Split, apply, combine</p></li>
<li><p>The GroupBy object</p>
<ul class="simple">
<li><p>Column indexing</p></li>
<li><p>Iteration over groups</p></li>
<li><p>Dispatch methods</p></li>
</ul>
</li>
<li><p>Aggregate, filter, transform, apply</p>
<ul>
<li><p>Aggregation</p></li>
<li><p>Filtering</p></li>
<li><p>Transformation</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">apply()</span></code> method</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">apply()</span></code> method lets you apply an arbitrary function to the group results. The function should take a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, and return either a Pandas object (e.g., <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>, <code class="docutils literal notranslate"><span class="pre">Series</span></code>) or a scalar; the combine operation will be tailored to the type of output returned.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;key&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">norm_by_data2</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Specifying the split key</p>
<ul class="simple">
<li><p>A list, array, series, or index providing the grouping keys</p></li>
<li><p>A dictionary or series mapping index to group</p></li>
<li><p>Any Python function</p></li>
<li><p>A list of valid keys</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Grouping example</p>
  </details>
</li>
</ul>
</div>
<div class="section" id="pivotal-tables">
<h3>Pivotal tables<a class="headerlink" href="#pivotal-tables" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Motivating Pivot Tables</p>
<ul class="simple">
<li><p>The pivot table takes simple column-wise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; It helpsto think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you split-apply-combine, but both the split and the combine happen across not a one-dimensional index, but across a two-dimensional grid.</p></li>
</ul>
</li>
<li><p>Pivot Tables by Hand</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titanic</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="s1">&#39;class&#39;</span><span class="p">])[</span><span class="s1">&#39;survived&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Pivot Table Syntax</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">titanic</span><span class="o">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;class&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>Multi-level pivot tables</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">age</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">titanic</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
<span class="n">titanic</span><span class="o">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="s1">&#39;survived&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">,</span> <span class="n">age</span><span class="p">],</span> <span class="s1">&#39;class&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Additional pivot table options</p></li>
</ul>
</li>
<li><p>Example: Birthrate DataFurther data exploration¶</p>
<ul class="simple">
<li><p>Further data exploration</p>
<ul>
<li><p>You can learn more about sigma-clipping operations in a book I coauthored with Željko Ivezić, Andrew J. Connolly, and Alexander Gray: “<em>Statistics, Data Mining, and Machine Learning in Astronomy” (Princeton University Press, 2014)</em></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="vectorized-string-operations">
<h3>Vectorized String Operations<a class="headerlink" href="#vectorized-string-operations" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Introducing Pandas String Operations</p>
<ul class="simple">
<li><p>This <em>vectorization</em> of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you’re stuck using a more verbose loop syntax</p></li>
</ul>
</li>
<li><p>Tables of Pandas String Methods</p>
<ul>
<li><p>Methods similar to Python string methods</p>
<ul class="simple">
<li><p>Nearly all Python’s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas <code class="docutils literal notranslate"><span class="pre">str</span></code> methods that mirror Python string methods
<code class="docutils literal notranslate"><span class="pre">len()</span></code>      <code class="docutils literal notranslate"><span class="pre">lower()</span> </code>      <code class="docutils literal notranslate"><span class="pre">translate()</span></code>   <code class="docutils literal notranslate"><span class="pre">islower()</span></code><br />
<code class="docutils literal notranslate"><span class="pre">ljust()</span></code>    <code class="docutils literal notranslate"><span class="pre">upper()</span></code>       <code class="docutils literal notranslate"> <span class="pre">startswith()</span></code>  <code class="docutils literal notranslate"><span class="pre">isupper()</span></code><br />
<code class="docutils literal notranslate"><span class="pre">rjust()</span></code>    <code class="docutils literal notranslate"><span class="pre">find()</span> </code>       <code class="docutils literal notranslate"><span class="pre">endswith()</span></code>     <code class="docutils literal notranslate"><span class="pre">isnumeric()</span></code><br />
<code class="docutils literal notranslate"> <span class="pre">center()</span></code>  <code class="docutils literal notranslate"><span class="pre">rfind()</span></code>       <code class="docutils literal notranslate"><span class="pre">isalnum()</span></code>      <code class="docutils literal notranslate"><span class="pre">isdecimal()</span></code><br />
<code class="docutils literal notranslate"><span class="pre">zfill()</span> </code>   <code class="docutils literal notranslate"><span class="pre">index()</span></code>       <code class="docutils literal notranslate"><span class="pre">isalpha()</span></code>      <code class="docutils literal notranslate"><span class="pre">split()</span></code><br />
<code class="docutils literal notranslate"><span class="pre">strip()</span></code>    <code class="docutils literal notranslate"><span class="pre">rindex()</span></code>      <code class="docutils literal notranslate"><span class="pre">isdigit()</span></code>      <code class="docutils literal notranslate"><span class="pre">rsplit()</span></code><br />
<code class="docutils literal notranslate"> <span class="pre">rstrip()</span></code>  <code class="docutils literal notranslate"><span class="pre">capitalize()</span></code>  <code class="docutils literal notranslate"><span class="pre">isspace()</span></code>      <code class="docutils literal notranslate"><span class="pre">partition()</span></code><br />
<code class="docutils literal notranslate"><span class="pre">lstrip()</span></code>   <code class="docutils literal notranslate"><span class="pre">swapcase()</span></code>    <code class="docutils literal notranslate"><span class="pre">istitle()</span></code>      <code class="docutils literal notranslate"><span class="pre">rpartition()</span></code></p></li>
</ul>
</li>
<li><p>Methods using regular expressions</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">match()</span></code></p></td>
<td><p>Call <code class="docutils literal notranslate"><span class="pre">re.match()</span></code> on each element, returning a boolean.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">extract()</span></code></p></td>
<td><p>Call <code class="docutils literal notranslate"><span class="pre">re.match()</span></code> on each element, returning matched groups as strings.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">findall()</span></code></p></td>
<td><p>Call <code class="docutils literal notranslate"><span class="pre">re.findall()</span></code> on each element</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">replace()</span></code></p></td>
<td><p>Replace occurrences of pattern with some other string</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">contains()</span></code></p></td>
<td><p>Call <code class="docutils literal notranslate"><span class="pre">re.search()</span></code> on each element, returning a boolean</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">count()</span></code></p></td>
<td><p>Count occurrences of pattern</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">split()</span></code></p></td>
<td><p>Equivalent to str.split(), but accepts regexps</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">rsplit()</span></code></p></td>
<td><p>Equivalent to str.rsplit(), but accepts regexps</p></td>
</tr>
</tbody>
</table>
</li>
<li><p>Miscellaneous methods</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">get()</span> </code></p></td>
<td><p>Index each element</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">slice()</span></code></p></td>
<td><p>Slice each element</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">slice_replace()</span></code></p></td>
<td><p>Replace slice in each element with passed value</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cat()</span></code></p></td>
<td><p>Concatenate strings</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">repeat()</span></code></p></td>
<td><p>Repeat values</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">normalize()</span></code></p></td>
<td><p>Return Unicode form of string</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">pad()</span></code></p></td>
<td><p>Add whitespace to left, right, or both sides of strings</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">wrap()</span></code></p></td>
<td><p>Split long strings into lines with length less than a given width</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">join()</span></code></p></td>
<td><p>Join strings in each element of the Series with passed separator</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">get_dummies()</span></code></p></td>
<td><p>extract dummy variables as a dataframe</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Vectorized item access and slicing</p></li>
<li><p>Indicator variables</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example: Recipe Database</p>
<ul class="simple">
<li><p>A simple recipe recommender</p></li>
<li><p>Going further with recipes</p></li>
</ul>
</li>
</ul>
</details>  
</div>
<div class="section" id="working-with-time-series">
<h3>Working with Time Series<a class="headerlink" href="#working-with-time-series" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Date and time data comes in a few flavors, which we will discuss here:</p>
<ul class="simple">
<li><p><em>Time stamps</em> reference particular moments in time (e.g., July 4th, 2015 at 7:00am).</p></li>
<li><p><em>Time intervals and periods</em> reference a length of time between a particular beginning and end point; for example, the year 2015. Periods usually reference a special case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods comprising days).</p></li>
<li><p><em>Time deltas</em> or <em>durations</em> reference an exact length of time (e.g., a duration of 22.56 seconds).</p></li>
</ul>
</li>
<li><p>Dates and Times in Python</p>
<ul>
<li><p>Native Python dates and times: <code class="docutils literal notranslate"><span class="pre">datetime</span></code> and <code class="docutils literal notranslate"><span class="pre">dateutil</span></code></p>
<ul class="simple">
<li><p>Python’s basic objects for working with dates and times reside in the built-in <code class="docutils literal notranslate"><span class="pre">datetime</span></code> module. Along with the third-party <code class="docutils literal notranslate"><span class="pre">dateutil</span></code> module, you can use it to quickly perform a host of useful functionalities on dates and times.</p></li>
<li><p>A related package to be aware of is <code class="docutils literal notranslate"><span class="pre">pytz</span></code>, which contains tools for working with the most migrane-inducing piece of time series data: time zones.</p></li>
</ul>
</li>
<li><p>Typed arrays of times: NumPy’s <code class="docutils literal notranslate"><span class="pre">datetime64</span></code></p>
<ul class="simple">
<li><p>The weaknesses of Python’s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> requires a very specific input format:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">date</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="s1">&#39;2015-07-04&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">datetime64</span><span class="p">)</span>
<span class="n">date</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">date</span><span class="p">(</span><span class="mi">2015</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;datetime64[D]&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>One detail of the <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> and <code class="docutils literal notranslate"><span class="pre">timedelta64</span></code> objects is that they are built on a fundamental time unit. Because the <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> object is limited to 64-bit precision, the range of encodable times is <span class="math notranslate nohighlight">\(2^{64}\)</span> times this fundamental unit. In other words, <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> imposes a trade-off between time resolution and maximum time span.</p></li>
<li><p>The time zone is automatically set to the local time on the computer executing the code. You can force any desired fundamental unit using one of many format codes</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Code</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Time span (relative)</p></th>
<th class="head"><p>Time span (absolute)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Y</p></td>
<td><p>Year</p></td>
<td><p>± 9.2e18 years</p></td>
<td><p>[9.2e18 BC, 9.2e18 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>M</p></td>
<td><p>Month</p></td>
<td><p>± 7.6e17 years</p></td>
<td><p>[7.6e17 BC, 7.6e17 AD]</p></td>
</tr>
<tr class="row-even"><td><p>W</p></td>
<td><p>Week</p></td>
<td><p>± 1.7e17 years</p></td>
<td><p>[1.7e17 BC, 1.7e17 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>Day</p></td>
<td><p>± 2.5e16 years</p></td>
<td><p>[2.5e16 BC, 2.5e16 AD]</p></td>
</tr>
<tr class="row-even"><td><p>h</p></td>
<td><p>Hour</p></td>
<td><p>± 1.0e15 years</p></td>
<td><p>[1.0e15 BC, 1.0e15 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>m</p></td>
<td><p>Minute</p></td>
<td><p>± 1.7e13 years</p></td>
<td><p>[1.7e13 BC, 1.7e13 AD]</p></td>
</tr>
<tr class="row-even"><td><p>s</p></td>
<td><p>Second</p></td>
<td><p>± 2.9e12 years</p></td>
<td><p>[ 2.9e9 BC, 2.9e9 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>ms</p></td>
<td><p>Millisecond</p></td>
<td><p>± 2.9e9 years</p></td>
<td><p>[ 2.9e6 BC, 2.9e6 AD]</p></td>
</tr>
<tr class="row-even"><td><p>us</p></td>
<td><p>Microsecond</p></td>
<td><p>± 2.9e6 years</p></td>
<td><p>[290301 BC, 294241 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>ns</p></td>
<td><p>Nanosecond</p></td>
<td><p>± 292 years</p></td>
<td><p>[ 1678 AD, 2262 AD]</p></td>
</tr>
<tr class="row-even"><td><p>ps</p></td>
<td><p>Picosecond</p></td>
<td><p>± 106 days</p></td>
<td><p>[ 1969 AD, 1970 AD]</p></td>
</tr>
<tr class="row-odd"><td><p>fs</p></td>
<td><p>Femtosecond</p></td>
<td><p>± 2.6 hours</p></td>
<td><p>[ 1969 AD, 1970 AD]</p></td>
</tr>
<tr class="row-even"><td><p>as</p></td>
<td><p>Attosecond</p></td>
<td><p>± 9.2 seconds</p></td>
<td><p>[ 1969 AD, 1970 AD]</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>For the types of data we see in the real world, a useful default is <code class="docutils literal notranslate"><span class="pre">datetime64[ns]</span></code>, as it can encode a useful range of modern dates with a suitably fine precision.</p></li>
<li><p>we will note that while the <code class="docutils literal notranslate"><span class="pre">datetime64</span></code> data type addresses some of the deficiencies of the built-in Python datetime type, it lacks many of the convenient methods and functions provided by <code class="docutils literal notranslate"><span class="pre">datetime</span></code> and especially <code class="docutils literal notranslate"><span class="pre">dateutil</span></code>.</p></li>
</ul>
</li>
<li><p>Dates and times in pandas: best of both worlds</p>
<ul class="simple">
<li><p>Pandas builds upon all the tools just discussed to provide a <code class="docutils literal notranslate"><span class="pre">Timestamp</span></code> object, which combines the ease-of-use of <code class="docutils literal notranslate"><span class="pre">datetime</span></code> and <code class="docutils literal notranslate"><span class="pre">dateutil</span></code> with the efficient storage and vectorized interface of <code class="docutils literal notranslate"><span class="pre">numpy.datetime64</span></code>. From a group of these <code class="docutils literal notranslate"><span class="pre">Timestamp</span></code> objects, Pandas can construct a <code class="docutils literal notranslate"><span class="pre">DatetimeIndex</span></code> that can be used to index data in a <code class="docutils literal notranslate"><span class="pre">Series</span></code> or <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code></p></li>
</ul>
</li>
<li><p>The power of datetime and dateutil lie in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are suboptimal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates.</p></li>
</ul>
</li>
<li><p>Pandas Time Series: Indexing by Time</p>
<ul class="simple">
<li><p>Where the Pandas time series tools really become useful is when you begin to <em>index data by timestamps</em>.</p></li>
</ul>
</li>
<li><p>Pandas Time Series Data Structures</p>
<ul>
<li><p>For <em>time stamps</em>, Pandas provides the <code class="docutils literal notranslate"><span class="pre">Timestamp</span></code> type. The associated Index structure is <code class="docutils literal notranslate"><span class="pre">DatetimeIndex</span></code>.</p></li>
<li><p>For <em>time Periods</em>, Pandas provides the <code class="docutils literal notranslate"><span class="pre">Period</span></code> type. This encodes a fixed-frequency interval based on <code class="docutils literal notranslate"><span class="pre">numpy.datetime64</span></code>. The associated index structure is <code class="docutils literal notranslate"><span class="pre">PeriodIndex</span></code>.</p></li>
<li><p>For <em>time deltas or durations</em>, Pandas provides the <code class="docutils literal notranslate"><span class="pre">Timedelta</span></code> type. <code class="docutils literal notranslate"><span class="pre">Timedelta</span></code> is a more efficient replacement for Python’s native <code class="docutils literal notranslate"><span class="pre">datetime.timedelta</span></code> type, and is based on <code class="docutils literal notranslate"><span class="pre">numpy.timedelta64</span></code>. The associated index structure is <code class="docutils literal notranslate"><span class="pre">TimedeltaIndex</span></code>.</p></li>
<li><p>The most fundamental of these date/time objects are the <code class="docutils literal notranslate"><span class="pre">Timestamp</span></code> and <code class="docutils literal notranslate"><span class="pre">DatetimeIndex</span></code> objects. While these class objects can be invoked directly, it is more common to use the <code class="docutils literal notranslate"><span class="pre">pd.to_datetime()</span></code> function, which can parse a wide variety of formats. Passing a single date to <code class="docutils literal notranslate"><span class="pre">pd.to_datetime()</span></code> yields a <code class="docutils literal notranslate"><span class="pre">Timestamp</span></code>; passing a series of dates by default yields a <code class="docutils literal notranslate"><span class="pre">DatetimeIndex</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dates</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">([</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2015</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;4th of July, 2015&#39;</span><span class="p">,</span>
                 <span class="s1">&#39;2015-Jul-6&#39;</span><span class="p">,</span> <span class="s1">&#39;07-07-2015&#39;</span><span class="p">,</span> <span class="s1">&#39;20150708&#39;</span><span class="p">])</span>
<span class="n">dates</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">DatetimeIndex</span><span class="p">([</span><span class="s1">&#39;2015-07-03&#39;</span><span class="p">,</span> <span class="s1">&#39;2015-07-04&#39;</span><span class="p">,</span> <span class="s1">&#39;2015-07-06&#39;</span><span class="p">,</span> <span class="s1">&#39;2015-07-07&#39;</span><span class="p">,</span>
         <span class="s1">&#39;2015-07-08&#39;</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;datetime64[ns]&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>A TimedeltaIndex is created, for example, when a date is subtracted from another:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dates</span> <span class="o">-</span> <span class="n">dates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">TimedeltaIndex</span><span class="p">([</span><span class="s1">&#39;0 days&#39;</span><span class="p">,</span> <span class="s1">&#39;1 days&#39;</span><span class="p">,</span> <span class="s1">&#39;3 days&#39;</span><span class="p">,</span> <span class="s1">&#39;4 days&#39;</span><span class="p">,</span> <span class="s1">&#39;5 days&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;timedelta64[ns]&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Regular sequences: <code class="docutils literal notranslate"><span class="pre">pd.date_range()</span></code></p></li>
</ul>
</li>
<li><p>Frequencies and Offsets</p>
<ul class="simple">
<li><p>Fundamental to these Pandas time series tools is the concept of a frequency or date offset.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Code</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Code</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>D</p></td>
<td><p>Calendar day</p></td>
<td><p>B</p></td>
<td><p>Business day</p></td>
</tr>
<tr class="row-odd"><td><p>W</p></td>
<td><p>Weekly</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>M</p></td>
<td><p>Month end</p></td>
<td><p>BM</p></td>
<td><p>Business month end</p></td>
</tr>
<tr class="row-odd"><td><p>Q</p></td>
<td><p>Quarter end</p></td>
<td><p>BQ</p></td>
<td><p>Business quarter end</p></td>
</tr>
<tr class="row-even"><td><p>A</p></td>
<td><p>Year end</p></td>
<td><p>BA</p></td>
<td><p>Business year end</p></td>
</tr>
<tr class="row-odd"><td><p>H</p></td>
<td><p>Hours</p></td>
<td><p>BH</p></td>
<td><p>Business hours</p></td>
</tr>
<tr class="row-even"><td><p>T</p></td>
<td><p>Minutes</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>S</p></td>
<td><p>Seconds</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>L</p></td>
<td><p>Milliseonds</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>U</p></td>
<td><p>Microseconds</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>N</p></td>
<td><p>nanoseconds</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>The monthly, quarterly, and annual frequencies are all marked at the end of the specified period. By adding an S suffix to any of these, they instead will be marked at the beginning:</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Code</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Code</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MS</p></td>
<td><p>Month start</p></td>
<td><p>BMS</p></td>
<td><p>Business month start</p></td>
</tr>
<tr class="row-odd"><td><p>QS</p></td>
<td><p>Quarter start</p></td>
<td><p>BQS</p></td>
<td><p>Business quarter start</p></td>
</tr>
<tr class="row-even"><td><p>AS</p></td>
<td><p>Year start</p></td>
<td><p>BAS</p></td>
<td><p>Business year start</p></td>
</tr>
</tbody>
</table>
</li>
<li><p>Resampling, Shifting, and Windowing</p>
<ul class="simple">
<li><p>Resampling and converting frequencies</p>
<ul>
<li><p>One common need for time series data is resampling at a higher or lower frequency. This can be done using the <code class="docutils literal notranslate"><span class="pre">resample()</span></code> method, or the much simpler <code class="docutils literal notranslate"><span class="pre">asfreq()</span></code> method. The primary difference between the two is that <code class="docutils literal notranslate"><span class="pre">resample()</span></code> is fundamentally a data aggregation, while <code class="docutils literal notranslate"><span class="pre">asfreq()</span></code> is fundamentally a data selection.</p></li>
</ul>
</li>
<li><p>Time-shifts</p></li>
<li><p>Rolling windows</p></li>
</ul>
</li>
<li><p>Where to Learn More</p></li>
<li><p>Example: Visualizing Seattle Bicycle Counts</p>
<ul class="simple">
<li><p>Visualizing the data</p></li>
<li><p>Digging into the data</p></li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="high-performance-pandas-eval-and-query">
<h3>High-Performance Pandas: eval() and query()<a class="headerlink" href="#high-performance-pandas-eval-and-query" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>As of version 0.13 (released January 2014), Pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of intermediate arrays. These are the <code class="docutils literal notranslate"><span class="pre">eval()</span></code> and <code class="docutils literal notranslate"><span class="pre">query()</span></code> functions, which rely on the <code class="docutils literal notranslate"><span class="pre">Numexpr</span> </code>package.</p></li>
<li><p>Motivating <code class="docutils literal notranslate"><span class="pre">query()</span></code> and <code class="docutils literal notranslate"><span class="pre">eval()</span></code>: Compound Expressions</p>
<ul class="simple">
<li><p>In other words, every intermediate step is explicitly allocated in memory. If the <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> arrays are very large, this can lead to significant memory and computational overhead. The <code class="docutils literal notranslate"><span class="pre">Numexpr</span></code> library gives you the ability to compute this type of compound expression element by element, without the need to allocate full intermediate arrays.</p></li>
<li><p>The benefit here is that <code class="docutils literal notranslate"><span class="pre">Numexpr</span> </code>evaluates the expression in a way that does not use full-sized temporary arrays, and thus can be much more efficient than NumPy, especially for large arrays. The Pandas <code class="docutils literal notranslate"><span class="pre">eval()</span></code> and <code class="docutils literal notranslate"><span class="pre">query()</span></code> tools that we will discuss here are conceptually similar, and depend on the Numexpr package.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">pandas.eval()</span></code> for Efficient Operations</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">eval()</span></code> function in Pandas uses string expressions to efficiently compute operations using DataFrames</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s1">&#39;df1 + df2 + df3 + df4&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Operations supported by <code class="docutils literal notranslate"><span class="pre">pd.eval()</span></code></p>
<ul class="simple">
<li><p>Comparison operators</p></li>
<li><p>Bitwise operators</p></li>
<li><p>Object attributes and indices</p></li>
<li><p>Other operations</p></li>
</ul>
</li>
</ul>
</li>
<li><p>DataFrame.eval() for Column-Wise Operations</p>
<ul class="simple">
<li><p>Just as Pandas has a top-level <code class="docutils literal notranslate"><span class="pre">pd.eval()</span></code> function, DataFrames have an <code class="docutils literal notranslate"><span class="pre">eval()</span></code> method that works in similar ways. The benefit of the <code class="docutils literal notranslate"><span class="pre">eval()</span></code> method is that columns can be referred to by name:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s2">&quot;(df.A + df.B) / (df.C - 1)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>Assignment in <code class="docutils literal notranslate"><span class="pre">DataFrame.eval()</span></code></p>
<ul>
<li><p>In addition to the options just discussed, <code class="docutils literal notranslate"><span class="pre">DataFrame.eval()</span></code> also allows assignment to any column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="s1">&#39;D = (A - B) / C&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># compute D based on A, B, and C</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Local variables in <code class="docutils literal notranslate"><span class="pre">DataFrame.eval()</span></code></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">DataFrame.eval()</span></code> method supports an additional syntax that lets it work with local Python variables.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> character here marks a variable name rather than a <em>column name</em>, and lets you efficiently evaluate expressions involving the two “namespaces”: the namespace of columns, and the namespace of Python objects. Notice that this <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> character is only supported by the <code class="docutils literal notranslate"><span class="pre">DataFrame.eval()</span></code> method, not by the <code class="docutils literal notranslate"><span class="pre">pandas.eval()</span> </code>function, because the <code class="docutils literal notranslate"><span class="pre">pandas.eval()</span></code> function only has access to the one (Python) namespace.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">DataFrame.query()</span></code> Method</p>
<ul>
<li><p>Note that the <code class="docutils literal notranslate"><span class="pre">query()</span></code> method also accepts the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> flag to mark local variables</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Cmean</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">A</span> <span class="o">&lt;</span> <span class="n">Cmean</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">B</span> <span class="o">&lt;</span> <span class="n">Cmean</span><span class="p">)]</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;A &lt; @Cmean and B &lt; @Cmean&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result1</span><span class="p">,</span> <span class="n">result2</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Performance: When to Use These Functions</p>
<ul class="simple">
<li><p>When considering whether to use these functions, there are two considerations: computation time and memory use. Memory use is the most predictable aspect. As already mentioned, every compound expression involving NumPy arrays or Pandas DataFrames will result in implicit creation of temporary arrays</p></li>
<li><p>If the size of the temporary DataFrames is significant compared to your available system memory (typically several gigabytes) then it’s a good idea to use an eval() or query() expression.</p></li>
<li><p>On the performance side, <code class="docutils literal notranslate"><span class="pre">eval()</span></code> can be faster even when you are not maxing-out your system memory. The issue is how your temporary DataFrames compare to the size of the L1 or L2 CPU cache on your system (typically a few megabytes in 2016); if they are much bigger, then <code class="docutils literal notranslate"><span class="pre">eval()</span></code> can avoid some potentially slow movement of values between the different memory caches. In practice, I find that the difference in computation time between the traditional methods and the eval/query method is usually not significant–if anything, the traditional method is faster for smaller arrays! The benefit of eval/query is mainly in the saved memory, and the sometimes cleaner syntax they offer.</p></li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="further-resources">
<h3>Further Resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://pandas.pydata.org/">Python Data Analysis Library — pandas: Python Data Analysis Library</a></p></li>
<li><p><a class="reference external" href="http://shop.oreilly.com/product/0636920023784.do">Python for Data Analysis - O’Reilly Media</a></p></li>
<li><p><a class="reference external" href="http://stackoverflow.com/questions/tagged/pandas">Newest ‘pandas’ Questions - Stack Overflow</a></p></li>
<li><p><a class="reference external" href="http://pyvideo.org/search?q=pandas">PyVideo.org</a></p></li>
</ul>
<hr class="docutils" />
</div>
</div>
<div class="section" id="machine-learning-computer">
<h2>3. Machine Learning :computer:<a class="headerlink" href="#machine-learning-computer" title="Permalink to this headline">¶</a></h2>
<div class="section" id="what-is-machine-learning">
<h3>What Is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The study of machine learning certainly arose from research in this context, but in the data science application of machine learning methods, it’s more helpful to think of machine learning as a means of <em>building models of data</em>.</p></li>
<li><p>Fundamentally, machine learning involves building mathematical models to help understand data. “Learning” enters the fray when we give these models <em>tunable parameters</em> that can be adapted to observed data; in this way the program can be considered to be “learning” from the data. Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data.</p></li>
<li><p>Categories of Machine Learning</p>
<ul>
<li><p><em>Supervised learning</em> involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data.</p>
<ul>
<li><p>Classification</p></li>
<li><p>Regression</p></li>
</ul>
</li>
<li><p><em>Unsupervised learning</em> involves modeling the features of a dataset without reference to any label, and is often described as “letting the dataset speak for itself.”</p>
<ul>
<li><p>Clustering</p></li>
<li><p>Dimensionality reduction</p></li>
</ul>
</li>
<li><p>In addition, there are so-called <em>semi-supervised learning</em> methods, which falls somewhere between supervised learning and unsupervised learning. Semi-supervised learning methods are often useful when only incomplete labels are available.</p></li>
</ul>
</li>
<li><p>Qualitative Examples of Machine Learning Applications</p>
<ul>
<li><p>Classification: Predicting discrete labels</p></li>
<li><p>Regression: Predicting continuous labels</p></li>
<li><p>Clustering: Inferring labels on unlabeled data</p></li>
<li><p>Dimensionality reduction: Inferring structure of unlabeled data</p></li>
</ul>
</li>
<li><p>Summary</p></li>
</ul>
</div>
<div class="section" id="introducing-scikit-learn">
<h3>Introducing Scikit-Learn<a class="headerlink" href="#introducing-scikit-learn" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Data Representation in Scikit-Learn</p>
<ul>
<li><p>The best way to think about data within Scikit-Learn is in terms of tables of data.</p></li>
<li><p>Data as table</p>
<ul>
<li><p>Features matrix</p></li>
<li><p>Target array</p>
<ul>
<li><p>Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to <em>predict from the data</em>: in statistical terms, it is the dependent variable.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Scikit-Learn’s Estimator API</p>
<ul>
<li><p>The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the Scikit-Learn API paper:</p>
<ul>
<li><p><em>Consistency</em>: All objects share a common interface drawn from a limited set of methods, with consistent documentation.</p></li>
<li><p><em>Inspection</em>: All specified parameter values are exposed as public attributes.</p></li>
<li><p><em>Limited object hierarchy</em>: Only algorithms are represented by Python classes; datasets are represented in standard formats (NumPy arrays, Pandas <code class="docutils literal notranslate"><span class="pre">DataFrames</span></code>, SciPy sparse matrices) and parameter names use standard Python strings.</p></li>
<li><p><em>Composition</em>: Many machine learning tasks can be expressed as sequences of more fundamental algorithms, and Scikit-Learn makes use of this wherever possible.</p></li>
<li><p><em>Sensible defaults</em>: When models require user-specified parameters, the library defines an appropriate default value.</p></li>
</ul>
</li>
<li><p>In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood. Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.</p></li>
<li><p>Basics of the API</p>
<ul>
<li><p>Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).</p>
<ol class="simple">
<li><p>Choose a class of model by importing the appropriate estimator class from Scikit-Learn.</p></li>
<li><p>Choose model hyperparameters by instantiating this class with desired values.</p></li>
<li><p>Arrange data into a features matrix and target vector following the discussion above.</p></li>
<li><p>Fit the model to your data by calling the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method of the model instance.</p></li>
<li><p>Apply the Model to new data:</p>
<ul>
<li><p>For supervised learning, often we predict labels for unknown data using the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method.</p></li>
<li><p>For unsupervised learning, we often transform or infer properties of the data using the <code class="docutils literal notranslate"><span class="pre">transform()</span></code> or <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method.</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>Supervised learning example: Simple linear regression</p>
<ol class="simple">
<li><p>Choose a class of model</p></li>
<li><p>Choose model hyperparameters</p>
<ul>
<li><p>An important point is that a class of model is not the <em>same as an instance of a model</em>.</p></li>
<li><p>In Scikit-Learn, by convention all model parameters that were learned during the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> process have trailing underscores</p></li>
<li><p>Keep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values. In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between <em>choice of model</em> and <em>application of model to data</em>.</p></li>
</ul>
</li>
<li><p>Arrange data into a features matrix and target vector</p>
<ul>
<li><p>In general, Scikit-Learn does not provide tools to draw conclusions from internal model parameters themselves: interpreting model parameters is much more a <em>statistical modeling</em> question than a <em>machine learning</em> question.</p></li>
</ul>
</li>
<li><p>Fit the model to your data</p></li>
<li><p>Predict labels for unknown data</p></li>
</ol>
</li>
<li><p>Supervised learning example: Iris classification</p>
<ul>
<li><p>Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models.</p></li>
</ul>
</li>
<li><p>Unsupervised learning example: Iris dimensionality</p>
<ul>
<li><p>Often dimensionality reduction is used as an aid to visualizing data: after all, it is much easier to plot data in two dimensions than in four dimensions or higher!</p></li>
</ul>
</li>
<li><p>Unsupervised learning: Iris clustering</p></li>
</ul>
</li>
<li><p>Application: Exploring Hand-written Digits</p>
<ul>
<li><p>Loading and visualizing the digits data</p></li>
<li><p>Unsupervised learning: Dimensionality reduction</p></li>
<li><p>Classification on digits</p></li>
</ul>
</li>
<li><p>Summary</p></li>
</ul>
</details>
</div>
<div class="section" id="hyperparameters-and-model-validation">
<h3>Hyperparameters and Model Validation<a class="headerlink" href="#hyperparameters-and-model-validation" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Thinking about Model Validation</p>
<ul>
<li><p>Model validation the wrong way</p></li>
<li><p>Model validation the right way: Holdout sets</p></li>
<li><p>Model validation via cross-validation</p>
<ul>
<li><p>One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training.</p></li>
<li><p>One way to address this is to use <em>cross-validation</em>; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.</p></li>
<li><p>Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the <code class="docutils literal notranslate"><span class="pre">cross_validation</span></code> module.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Selecting the Best Model</p>
<ul>
<li><p>Of core importance is the following question: <em>if our estimator is underperforming, how should we move forward?</em> There are several possible answers:</p>
<ul>
<li><p>Use a more complicated/more flexible model</p></li>
<li><p>Use a less complicated/less flexible model</p></li>
<li><p>Gather more training samples</p></li>
<li><p>Gather more data to add features to each sample</p></li>
</ul>
</li>
<li><p>The answer to this question is often counter-intuitive. In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results! The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.</p></li>
<li><p>The Bias-variance trade-off</p>
<ul>
<li><p>For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.</p></li>
<li><p>For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.</p></li>
</ul>
</li>
<li><p>Validation curves in Scikit-Learn</p></li>
</ul>
</li>
<li><p>Learning Curves</p>
<ul>
<li><p>We see that the behavior of the validation curve has not one but two important inputs: <strong>the model complexity</strong> and <strong>the number of training points</strong>. It is often useful to to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model. A plot of the training/validation score with respect to the size of the training set is known as a <em>learning curve</em>.</p></li>
<li><p>One important aspect of model complexity is that the optimal model will generally depend on the size of your training data.</p></li>
<li><p>The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows. In particular, once you have enough points that a particular model has converged, adding more training data will not help you! The only way to increase model performance in this case is to use another (often more complex) model.</p></li>
<li><p>The general behavior we would expect from a learning curve is this:</p>
<ul>
<li><p>A model of a given complexity will overfit a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.</p></li>
<li><p>A model of a given complexity will underfit a large dataset: this means that the training score will decrease, but the validation score will increase.</p></li>
<li><p>A model will never, except by chance, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross.</p></li>
</ul>
</li>
<li><p>Learning curves in Scikit-Learn</p>
<ul>
<li><p>This (learning curve) is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data. In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other) <em>adding more training data will not significantly improve the fit</em>!</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Validation in Practice: Grid Search</p>
<ul>
<li><p>The grid search provides many more options, including the ability to specify a custom scoring function, to parallelize the computations, to do randomized searches, and more.</p></li>
</ul>
</li>
<li><p>Summary</p></li>
</ul>
</details> 
</div>
<div class="section" id="feature-engineering">
<h3>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary>TOC</summary>
<ul>
<li><p>Categorical Features</p>
<ul class="simple">
<li><p>There is one clear disadvantage of this approach (one-hot encoding): if your category has many possible values, this can greatly increase the size of your dataset. However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></code> and <code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.FeatureHasher</span></code> are two additional tools that Scikit-Learn includes to support this type of encoding.</p></li>
</ul>
</li>
<li><p>Text Features</p>
<ul>
<li><p>There are some issues with this approach (using Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>), however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms. One approach to fix this is known as term frequency-inverse document frequency (TF–IDF) which weights the word counts by a measure of how often they appear in the documents</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Image Features</p>
<ul class="simple">
<li><p>A comprehensive summary of feature extraction techniques for images is well beyond the scope of this section, but you can find excellent implementations of many of the standard approaches in the <a class="reference external" href="http://scikit-image.org/">Scikit-Image project</a>.</p></li>
</ul>
</li>
<li><p>Derived Features</p>
<ul class="simple">
<li><p>This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.</p></li>
</ul>
</li>
<li><p>Imputation of Missing Data</p></li>
<li><p>Feature Pipelines</p>
<ul>
<li><p>This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the specified steps to any input data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">),</span>
                  <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                  <span class="n">LinearRegression</span><span class="p">())</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="in-depth-naive-bayes-classification">
<h3>In Depth: Naive Bayes Classification<a class="headerlink" href="#in-depth-naive-bayes-classification" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem.</p></li>
<li><p>Bayesian Classification</p>
<ul>
<li><p>This is where the “naive” in “naive Bayes” comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.</p></li>
<li><p>Different types of naive Bayes classifiers rest on different naive assumptions about the data</p></li>
</ul>
</li>
<li><p>Gaussian Naive Bayes</p>
<ul>
<li><p>Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In this classifier, the assumption is that <em>data from each label is drawn from a simple Gaussian distribution</em>.</p></li>
<li><p>A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method</p></li>
<li><p>in general, the boundary in Gaussian naive Bayes is quadratic</p></li>
</ul>
</li>
<li><p>Multinomial Naive Bayes</p>
<ul>
<li><p>The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.</p></li>
<li><p>Example: Classifying Text</p></li>
</ul>
</li>
<li><p>When to Use Naive Bayes</p>
<ul>
<li><p>Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages:</p>
<ul>
<li><p>They are extremely fast for both training and prediction</p></li>
<li><p>They provide straightforward probabilistic prediction</p></li>
<li><p>They are often very easily interpretable</p></li>
<li><p>They have very few (if any) tunable parameters</p></li>
</ul>
</li>
<li><p>Naive Bayes classifiers tend to perform especially well in one of the following situations:</p>
<ul>
<li><p>When the naive assumptions actually match the data (very rare in practice)</p></li>
<li><p>For very well-separated categories, when model complexity is less important</p></li>
<li><p>For very high-dimensional data, when model complexity is less important</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>  
</div>
<div class="section" id="in-depth-linear-regression">
<h3>In Depth: Linear Regression<a class="headerlink" href="#in-depth-linear-regression" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Simple Linear Regression</p></li>
<li><p>Basis Function Regression</p>
<ul>
<li><p>Notice that this is still a linear model—the linearity refers to the fact that the coefficients <span class="math notranslate nohighlight">\(a_n\)</span> never multiply or divide each other.</p></li>
<li><p>Polynomial basis functions</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>Gaussian basis functions</p>
<ul class="simple">
<li><p>one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. (add pic here)</p></li>
<li><p>This is typical over-fitting behavior when basis functions overlap: the coefficients of adjacent basis functions blow up and cancel each other out.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Regularization</p>
<ul>
<li><p>Ridge regression (<span class="math notranslate nohighlight">\(L_2\)</span> Regularization)</p>
<ul>
<li><p>Perhaps the most common form of regularization is known as <em>ridge regression</em> or <em><span class="math notranslate nohighlight">\(L_2\)</span> regularization</em>, sometimes also called <em>Tikhonov regularization</em>. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be
$<span class="math notranslate nohighlight">\(P = \alpha\sum_{n=1}^N \theta_n^2\)</span><span class="math notranslate nohighlight">\(
where \)</span>\alpha$ is a free parameter that controls the strength of the penalty.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Ridge Regression&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Lasso regression (<span class="math notranslate nohighlight">\(L_1\)</span> regularization)</p>
<ul class="simple">
<li><p>Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:
$<span class="math notranslate nohighlight">\(P = \alpha\sum_{n=1}^N|\theta_n|\)</span>$</p></li>
<li><p>due to geometric reasons lasso regression tends to favor sparse models where possible: that is, it preferentially sets model coefficients to exactly zero.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">GaussianFeatures</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">))</span>
<span class="n">basis_plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Lasso Regression&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Example: Predicting Bicycle Traffic</p></li>
</ul>
</details>
</div>
<div class="section" id="in-depth-support-vector-machines">
<h3>In-Depth: Support Vector Machines<a class="headerlink" href="#in-depth-support-vector-machines" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Motivating Support Vector Machines</p>
<ul class="simple">
<li><p><em>Generative classification</em>: when learning a simple model describing the distribution of each underlying class, and uses these generative models to probabilistically determine labels for new points .</p></li>
<li><p>In SVM we consider <em>discriminative classification</em>: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.</p></li>
</ul>
</li>
<li><p>Support Vector Machines: Maximizing the Margin</p>
<ul>
<li><p>Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a <em>margin</em> of some width, up to the nearest point.</p></li>
<li><p>In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a <em>maximum margin</em> estimator.</p></li>
<li><p>Fitting a support vector machine</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span> <span class="c1"># &quot;Support vector classifier&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E10</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A key to this classifier’s success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.</p></li>
</ul>
</li>
<li><p>Beyond linear boundaries: Kernel SVM</p>
<ul class="simple">
<li><p>Idea: when linear separation is not possible, project the data into a higher dimension such that a linear separator would be sufficient.</p></li>
<li><p>One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a <em>kernel transformation</em>, as it is based on a similarity relationship (or kernel) between each pair of points.</p></li>
<li><p>A potential problem with this strategy—projecting <span class="math notranslate nohighlight">\(N\)</span> points into <span class="math notranslate nohighlight">\(N\)</span> dimensions—is that it might become very computationally intensive as <span class="math notranslate nohighlight">\(N\)</span> grows large. However, because of a neat little procedure known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_trick">kernel trick</a>, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full <span class="math notranslate nohighlight">\(N\)</span>-dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.</p></li>
<li><p>In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter:</p></li>
</ul>
<div class="highlight-PYTHON notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1E6</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Tuning the SVM: Softening Margins</p>
<ul class="simple">
<li><p>the SVM implementation has a bit of a fudge-factor which “softens” the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.</p></li>
<li><p>The hardness of the margin is controlled by a tuning parameter, most often known as <span class="math notranslate nohighlight">\(C\)</span>. For very large <span class="math notranslate nohighlight">\(C\)</span>, the margin is hard, and points cannot lie in it. For smaller <span class="math notranslate nohighlight">\(C\)</span>, the margin is softer, and can grow to encompass some points.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example: Face Recognition</p></li>
<li><p>Support Vector Machine Summary</p>
<ul class="simple">
<li><p>Pros:</p>
<ul>
<li><p>Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.</p></li>
<li><p>Once the model is trained, the prediction phase is very fast.</p></li>
<li><p>Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.</p></li>
<li><p>Their integration with kernel methods makes them very versatile, able to adapt to many types of data.</p></li>
</ul>
</li>
<li><p>Cons:</p>
<ul>
<li><p>The scaling with the number of samples <span class="math notranslate nohighlight">\(N\)</span> is <span class="math notranslate nohighlight">\(\mathcal{O}[N^3]\)</span> at worst, or <span class="math notranslate nohighlight">\(\mathcal{O}[N^2]\)</span> for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.</p></li>
<li><p>The results are strongly dependent on a suitable choice for the softening parameter <span class="math notranslate nohighlight">\(C\)</span>. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.</p></li>
<li><p>The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the <code class="docutils literal notranslate"><span class="pre">probability</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">SVC</span></code>), but this extra estimation is costly.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="in-depth-decision-trees-and-random-forests">
<h3>In-Depth: Decision Trees and Random Forests<a class="headerlink" href="#in-depth-decision-trees-and-random-forests" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Random forests are an example of an <em>ensemble</em> method, meaning that it relies on aggregating the results of an ensemble of simpler estimators. The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts: that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting!</p></li>
<li><p>Motivating Random Forests: Decision Trees</p>
<ul>
<li><p>In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features.</p></li>
<li><p>Creating a decision tree</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Decision trees and over-fitting</p>
<ul class="simple">
<li><p>Over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Ensembles of Estimators: Random Forests</p>
<ul class="simple">
<li><p>Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Random Forest Regression</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Example: Random Forest for Classifying Digits</p></li>
<li><p>Summary of Random Forests</p>
<ul class="simple">
<li><p>Pros</p>
<ul>
<li><p>Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities.</p></li>
<li><p>The multiple trees allow for a probabilistic classification: a majority vote among estimators gives an estimate of the probability (accessed in Scikit-Learn with the <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> method).</p></li>
<li><p>The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>  
</div>
<div class="section" id="in-depth-principal-component-analysis">
<h3>In Depth: Principal Component Analysis<a class="headerlink" href="#in-depth-principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Introducing Principal Component Analysis</p>
<ul>
<li><p>PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The projection of each data point onto the principal axes are the “principal components” of the data. This transformation from data axes to principal axes is an <em>affine transformation</em>, which basically means it is composed of a translation, rotation, and uniform scaling.</p></li>
</ul>
</li>
<li><p>PCA as dimensionality reduction</p>
<ul class="simple">
<li><p>Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.</p></li>
</ul>
</li>
<li><p>PCA for visualization: Hand-written digits</p></li>
<li><p>What do the components mean?</p>
<ul class="simple">
<li><p>This meaning can be understood in terms of combinations of basis vectors.</p></li>
</ul>
</li>
<li><p>Choosing the number of components</p>
<ul>
<li><p>This can be determined by looking at the cumulative explained variance ratio as a function of the number of components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cumulative explained variance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p>PCA as Noise Filtering</p>
<ul class="simple">
<li><p>The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise. So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.</p></li>
</ul>
</li>
<li><p>Example: Eigenfaces</p></li>
<li><p>Principal Component Analysis Summary</p>
<ul class="simple">
<li><p>Cons:</p>
<ul>
<li><p>PCA’s main weakness is that it tends to be highly affected by outliers in the data.</p></li>
<li><p>While PCA is flexible, fast, and easily interpretable, it does not perform so well when there are nonlinear relationships within the data</p></li>
<li><p>For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components. Scikit-Learn contains a couple interesting variants on PCA, including <code class="docutils literal notranslate"><span class="pre">RandomizedPCA</span></code> and <code class="docutils literal notranslate"><span class="pre">SparsePCA</span></code>, both also in the <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition</span></code> submodule.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="in-depth-manifold-learning">
<h3>In-Depth: Manifold Learning<a class="headerlink" href="#in-depth-manifold-learning" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul class="simple">
<li><p>Here we will demonstrate a number of manifold methods, going most deeply into a couple techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (IsoMap).</p></li>
<li><p><em>manifold learning</em>—a class of unsupervised estimators that seeks to describe datasets as low-dimensional manifolds embedded in high-dimensional spaces.</p></li>
<li><p>Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.</p></li>
<li><p>Manifold learning algorithms would seek to learn about the fundamental two-dimensional nature of the paper, even as it is contorted to fill the three-dimensional space.</p></li>
<li><p>Manifold Learning: “HELLO”</p></li>
<li><p>Multidimensional Scaling (MDS)</p>
<ul>
<li><p>What the multidimensional scaling algorithm aims to do: given a distance matrix between points, it recovers a <span class="math notranslate nohighlight">\(D\)</span>-dimensional coordinate representation of the data.</p></li>
</ul>
</li>
<li><p>MDS as Manifold Learning</p>
<ul>
<li><p>The goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves certain relationships within the data. In the case of MDS, the quantity preserved is the distance between every pair of points.</p></li>
</ul>
</li>
<li><p>Nonlinear Embeddings: Where MDS Fails</p>
<ul>
<li><p>Where MDS breaks down is when the embedding is nonlinear—that is, when it goes beyond this simple set of operations.</p></li>
<li><p><em>linear</em> embeddings, which essentially consist of rotations, translations, and scalings of data into higher-dimensional spaces.</p></li>
</ul>
</li>
<li><p>Nonlinear Manifolds: Locally Linear Embedding</p>
<ul>
<li><p>locally linear embedding (LLE): rather than preserving all distances, it instead tries to preserve only the distances between neighboring points: in this case, the nearest 100 neighbors of each point.</p></li>
<li><p>Locally Linear Embedding (LLE) comes in a number of flavors; here we will use the modified LLE algorithm to recover the embedded two-dimensional manifold. In general, modified LLE does better than other flavors of the algorithm at recovering well-defined manifolds with very little distortion:</p></li>
</ul>
</li>
<li><p>Some Thoughts on Manifold Methods</p>
<ul>
<li><p>The following are some of the particular challenges of manifold learning, which all contrast poorly with PCA:</p>
<ul>
<li><p>In manifold learning, there is no good framework for handling missing data. In contrast, there are straightforward iterative approaches for missing data in PCA.</p></li>
<li><p>In manifold learning, the presence of noise in the data can “short-circuit” the manifold and drastically change the embedding. In contrast, PCA naturally filters noise from the most important components.</p></li>
<li><p>The manifold embedding result is generally highly dependent on the number of neighbors chosen, and there is generally no solid quantitative way to choose an optimal number of neighbors. In contrast, PCA does not involve such a choice.</p></li>
<li><p>In manifold learning, the globally optimal number of output dimensions is difficult to determine. In contrast, PCA lets you find the output dimension based on the explained variance.</p></li>
<li><p>In manifold learning, the meaning of the embedded dimensions is not always clear. In PCA, the principal components have a very clear meaning.</p></li>
<li><p>In manifold learning the computational expense of manifold methods scales as <span class="math notranslate nohighlight">\(O[N^2]\)</span> or <span class="math notranslate nohighlight">\(O[N^3]\)</span>. For PCA, there exist randomized approaches that are generally much faster (though see the <a class="reference external" href="https://github.com/mmp2/megaman">megaman</a> package for some more scalable implementations of manifold learning).</p></li>
</ul>
</li>
<li><p>the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data; for that reason I tend to explore data with manifold methods only after first exploring them with PCA.</p></li>
<li><p>Scikit-Learn implements several common variants of manifold learning beyond Isomap and LLE: the Scikit-Learn documentation has a <a class="reference external" href="http://scikit-learn.org/stable/modules/manifold.html">nice discussion and comparison of them</a>.</p>
<ul>
<li><p>For toy problems such as the S-curve we saw before, locally linear embedding (LLE) and its variants (especially <em>modified LLE</em>), perform very well. This is implemented in <code class="docutils literal notranslate"><span class="pre">sklearn.manifold.LocallyLinearEmbedding</span></code>.</p></li>
<li><p>For high-dimensional data from real-world sources, LLE often produces poor results, and isometric mapping (IsoMap) seems to generally lead to more meaningful embeddings. This is implemented in <code class="docutils literal notranslate"><span class="pre">sklearn.manifold.Isomap</span></code></p></li>
<li><p>For data that is highly clustered, <em>t-distributed stochastic neighbor embedding</em> (t-SNE) seems to work very well, though can be very slow compared to other methods. This is implemented in <code class="docutils literal notranslate"><span class="pre">sklearn.manifold.TSNE</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example: Isomap on Faces</p>
<ul>
<li><p>One place manifold learning is often used is in understanding the relationship between high-dimensional data points. A common case of high-dimensional data is images</p></li>
<li><p>We see that for this data, nearly 100 components are required to preserve 90% of the variance: this tells us that the data is intrinsically very high dimensional—it can’t be described linearly with just a few components.</p></li>
</ul>
</li>
<li><p>Example: Visualizing Structure in Digits</p>
<ul>
<li><p>Now, this in itself may not be useful for the task of classifying digits, but it does help us get an understanding of the data, and may give us ideas about how to move forward, such as how we might want to preprocess the data before building a classification pipeline.</p></li>
</ul>
</li>
</ul>
</details>
</div>
<div class="section" id="in-depth-k-means-clustering">
<h3>In Depth: k-Means Clustering<a class="headerlink" href="#in-depth-k-means-clustering" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>Introducing k-Means</p>
<ul class="simple">
<li><p>The <em>k</em>-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:</p>
<ul>
<li><p>The “cluster center” is the arithmetic mean of all the points belonging to the cluster.</p></li>
<li><p>Each point is closer to its own cluster center than to other cluster centers.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>k-Means Algorithm: Expectation–Maximization</p>
<ul>
<li><p>In short, the expectation–maximization approach here consists of the following procedure:</p>
<ol class="simple">
<li><p>Guess some cluster centers</p></li>
<li><p>Repeat until converged</p>
<ol class="simple">
<li><p><em>E-Step</em>: assign points to the nearest cluster center</p></li>
<li><p><em>M-Step</em>: set the cluster centers to the mean</p></li>
</ol>
</li>
</ol>
<ul class="simple">
<li><p>Here the “E-step” or “Expectation step” is so-named because it involves updating our expectation of which cluster each point belongs to. The “M-step” or “Maximization step” is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.</p></li>
</ul>
</li>
<li><p>Caveats of expectation–maximization</p>
<ul class="simple">
<li><p>The globally optimal result may not be achieved</p>
<ul>
<li><p>although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution.</p></li>
<li><p>For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the <code class="docutils literal notranslate"><span class="pre">n_init</span></code> parameter, which defaults to 10).</p></li>
</ul>
</li>
<li><p>The number of clusters must be selected beforehand</p>
<ul>
<li><p>Another common challenge with <em>k</em>-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.</p></li>
<li><p>one approach that is rather intuitive, but that we won’t discuss further here, is called <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">silhouette analysis</a>.</p></li>
<li><p>Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models; see In Depth: Gaussian Mixture Models) or which can choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the <code class="docutils literal notranslate"><span class="pre">sklearn.cluster</span></code> submodule)</p></li>
</ul>
</li>
<li><p>k-means is limited to linear cluster boundaries</p>
<ul>
<li><p>The fundamental model assumptions of <em>k</em>-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.</p></li>
<li><p>the boundaries between <em>k</em>-means clusters will always be linear, which means that it will fail for more complicated boundaries.</p></li>
<li><p>One version of this kernelized <em>k</em>-means is implemented in Scikit-Learn within the <code class="docutils literal notranslate"><span class="pre">SpectralClustering</span></code> estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a <em>k</em>-means algorithm</p></li>
</ul>
</li>
<li><p>k-means can be slow for large numbers of samples</p>
<ul>
<li><p>Because each iteration of <em>k</em>-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Examples</p>
<ul class="simple">
<li><p>Example 1: k-means on digits</p></li>
</ul>
</li>
<li><p>Example 2: k-means for color compression</p></li>
</ul>
</details>
</li>
</ul>
</div>
<div class="section" id="in-depth-gaussian-mixture-models">
<h3>In Depth: Gaussian Mixture Models<a class="headerlink" href="#in-depth-gaussian-mixture-models" title="Permalink to this headline">¶</a></h3>
<details open>
  <summary> TOC </summary>
<ul>
<li><p>In particular, the non-probabilistic nature of <em>k</em>-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.</p></li>
<li><p>Motivating GMM: Weaknesses of k-Means</p></li>
<li><p>Generalizing E–M: Gaussian Mixture Models</p>
<ul>
<li><p>A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as <em>k</em>-means</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GMM</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Under the hood, a Gaussian mixture model is very similar to <em>k</em>-means: it uses an expectation–maximization approach which qualitatively does the following:</p>
<ol class="simple">
<li><p>Choose starting guesses for the location and shape</p></li>
<li><p>Repeat until converged:</p>
<ol class="simple">
<li><p><em>E-step</em>: for each point, find weights encoding the probability of membership in each cluster</p></li>
<li><p><em>M-step</em>: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights</p></li>
</ol>
</li>
</ol>
</li>
<li><p>Choosing the covariance type</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">covariance_type</span></code> option was set differently within each. This hyperparameter controls the degrees of freedom in the shape of each cluster; it is essential to set this carefully for any given problem. The default is <code class="docutils literal notranslate"><span class="pre">covariance_type=&quot;diag&quot;</span></code>, which means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes. A slightly simpler and faster model is <code class="docutils literal notranslate"><span class="pre">covariance_type=&quot;spherical&quot;</span></code>, which constrains the shape of the cluster such that all dimensions are equal. The resulting clustering will have similar characteristics to that of k-means, though it is not entirely equivalent. A more complicated and computationally expensive model (especially as the number of dimensions grows) is to use <code class="docutils literal notranslate"><span class="pre">covariance_type=&quot;full&quot;</span></code>, which allows each cluster to be modeled as an ellipse with arbitrary orientation.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>GMM as Density Estimation</p>
<ul class="simple">
<li><p>Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for <em>density estimation</em>. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data.</p></li>
<li><p>How many components?</p>
<ul>
<li><p>The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the <em>likelihood</em> of the data under the model, using cross-validation to avoid over-fitting. Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). Scikit-Learn’s <code class="docutils literal notranslate"><span class="pre">GMM</span></code> estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach.</p></li>
<li><p>Notice the important point: this choice of number of components measures how well GMM works as a <em>density estimator</em>, not how well it works as a <em>clustering algorithm</em>. I’d encourage you to think of GMM primarily as a density estimator, and use it for clustering only when warranted within simple datasets.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example: GMM for Generating New Data</p></li>
</ul>
</details> 
<span class="target" id="id1"></span></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./DS_ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="Book-Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow.html" title="previous page">[book] Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By askming<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>