
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tree/Random Forest (Bagging) &#8212; Study Notes</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="HTML Basics" href="../CS/Note-HTML%20basics.html" />
    <link rel="prev" title="Stochastic Gradient Decent (SGD)" href="Note-Stochasitic%20Gradient%20Descent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Study Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Study Notes Index (knowledge base structure)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Analysis%20of%20Observational%20Health%20Care%20Data%20Using%20SAS.html">
   [book] Analysis of Observational Health Care Data Using SAS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Causal_inference_in_statistics_a_primer.html">
   [book] Causal inference in statistics
   <em>
    a primer
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Data_Analysis_Using_Regression_and_Multilevel_Hierarchical_Models.html">
   [book] Data Analysis Using Regression and Multilevel/Hierarchical Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Fundamentals%20of%20Clinical%20Trials%20%284th%20edition%29.html">
   [book] Fundamentals of Clinical Trials (4th edition)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Group%20Sequential%20and%20Confirmatory%20Adaptive%20Designs%20in%20Clinical%20Trials.html">
   [book] Group Sequential and Confirmatory Adaptive Designs in Clinical Trials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Book-Regression%20Modeling%20Strategies.html">
   [book] Regression Modeling Strategies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Adaptive%20Trial%20Design.html">
   Adaptive Clinical Trial Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Analysis%20of%20Ordinal%20Outcome.html">
   Analysis of ordinal outcome
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Analysis%20of%20Stratified%20Randomization%20Trial%20Data.html">
   Why analysis of stratified randomization trial data need to account for the stratification factors?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Association%20vs.%20Prediction%20vs.%20Causation.html">
   Association vs. Prediction vs. Causation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Baseline%20Adaptive%20Randomization.html">
   Baseline adaptive randomization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Causal%20inference%20general.html">
   Causal Inference General Study Notes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Competing_Risk_Regression.html">
   Competing risk models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Correlated%20Data%20Analysis.html">
   Correlated/Longitudinal Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Cox_PH_model.html">
   ​Cox PH model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Dirichlet%20process.html">
   Dirichlet process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-FDA%20guidances%20for%20industry.html">
   Notes on FDA Guidances for Industry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Gaussian%20process.html">
   Gaussain Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Group_Sequential_Trial_Design.html">
   Notes on Group Sequential Trial Design​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-ICH%20guidelines.html">
   Notes on ICH guidelines​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-MDD%20%26%20Effect%20size.html">
   Power,
   <strong>
    MDD
   </strong>
   /
   <strong>
    E
   </strong>
   (minimum detectable difference/effect), and
   <strong>
    Effect size
   </strong>
   in clinical trials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Meta-Analysis.html">
   Meta-analysis​
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Missing%20data.html">
   Missing data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Model%20comparison%20methods.html">
   Model comparison methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Monte%20Carlo%20method.html">
   Monte Carlo Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Multicolinearity.html">
   Multicolinearity
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Multiplicity%20in%20Clinical%20Trials.html">
   Multiplicity in Clinical Trials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Non-inferiority%20equivalence%20superiority%20test.html">
   Non-inferiority/equivalence/superiority test basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Observational%20study%20design.html">
   Observational study design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Propensity%20score%20methods.html">
   Propensity Score Methods &amp; Implementation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Quantile%20regression.html">
   Quantile regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Sample_size_calculation_with_WMW_test.html">
   Sample size calculation for the Wilcoxon-Mann-Whitney test adjusting for ties
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats/Note-Survival_Analysis.html">
   Survival Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistical computing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats_Comp/Note-How%20R%20searches%20and%20finds%20stuff.html">
   How R searches and finds stuff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats_Comp/Note-googleVis.html">
   Google R package googleVis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Stats_Comp/Note-matplotlib.html">
   Notes on
   <code class="docutils literal notranslate">
    <span class="pre">
     matplotlib
    </span>
   </code>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DS/ML
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Book-Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow.html">
   [book] Hands-On Machine Learning with Scikit-Learn and TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Book-Python%20Data%20Science%20Handbook.html">
   [book] Python Data Science Handbook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Note-Boosting.html">
   Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Note-Cluster%20analysis.html">
   Cluster analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Note-Machine%20Learning%20%28General%29.html">
   ​Machine Learning (General)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Note-Naive%20Bayes.html">
   Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Note-Stochasitic%20Gradient%20Descent.html">
   Stochastic Gradient Decent (SGD)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tree/Random Forest (Bagging)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-HTML%20basics.html">
   HTML Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-Hard%20SQL%20interview%20questions.html">
   Hard Data Analyst SQL Interview Questions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-Learning%20React.html">
   Learning React
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-Learning%20Rust.html">
   Learning Rust
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-SQL%20Study%20Notes.html">
   SQL study notes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CS/Note-git%20and%20github.html">
   git and github
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Miscellaneous/Miscellaneous%20Notes.html">
   Miscellaneous Study Notes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Miscellaneous/Phamaceutical%20development.html">
   Pharmacedutical development
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Miscellaneous/Sim-Coin%20flip.html">
   :bulb: Q: fair coin, # of flips to get two consecutive side (H or T)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Miscellaneous/Sim-Generate%20random%20number%20between%201%20to%207.html">
   Generate random number 1 to 7 using a fair dice
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/DS_ML/Note-Tree, Random Forest (Bagging).md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/askming/study_notes"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/askming/study_notes/issues/new?title=Issue%20on%20page%20%2FDS_ML/Note-Tree, Random Forest (Bagging).html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cart">
   CART
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-tree">
     Regression tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-tree">
     Classification tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trees-in-general">
     Trees in General
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tree-pruning">
     Tree Pruning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bagging">
   Bagging
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-variable-importance-in-rf">
       Defining variable importance in RF
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tree/Random Forest (Bagging)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cart">
   CART
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-tree">
     Regression tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-tree">
     Classification tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trees-in-general">
     Trees in General
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tree-pruning">
     Tree Pruning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bagging">
   Bagging
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-variable-importance-in-rf">
       Defining variable importance in RF
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="tree-random-forest-bagging">
<h1>Tree/Random Forest (Bagging)<a class="headerlink" href="#tree-random-forest-bagging" title="Permalink to this headline">¶</a></h1>
<div class="section" id="cart">
<h2>CART<a class="headerlink" href="#cart" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>CART is binary: each node has either 2 children or 0 children</p>
<ul>
<li><p>Other algorithm such as <strong>ID3</strong> can produce decision trees with nodes have more than two children</p></li>
<li><p>Other types of splitting rules</p>
<ul>
<li><p><strong>oblique decision trees</strong> or <strong>binary space partition tree</strong> (BSP trees): linear split at each node</p></li>
<li><p><strong>sphere trees</strong>: space is partitioned by a sphere of a certain radius around a fixed point</p></li>
</ul>
</li>
</ul>
</li>
<li><p>The CART algorithm searches for the pair of <span class="math notranslate nohighlight">\((k, t_k)\)</span>, a feature and its cutoff point, that produces the purest subsets (weighted by their size)</p>
<ul>
<li><p>CART algorithm is a greedy algorithm: it greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. it is not guaranteed to be the optimal solution.</p></li>
<li><p>finding the optimal tree is known to be an NP-Complete problem: it requires <span class="math notranslate nohighlight">\(O(\exp(m))\)</span> time, making the problem intractable even for fairly small training sets.</p></li>
</ul>
</li>
<li><p>Prediction complexity: Since each node only requires checking the value of one feature, the over all prediction complexity is just <span class="math notranslate nohighlight">\(O(\log2(m))\)</span>, independent of the number of features</p></li>
</ul>
<div class="section" id="regression-tree">
<h3>Regression tree<a class="headerlink" href="#regression-tree" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>If the loss function is <span class="math notranslate nohighlight">\(l(\hat{y}, y) = (\hat{y} - y)^2\)</span>, then the best choice of score on each leaf is <span class="math notranslate nohighlight">\(\hat{c}_m = avg(y_i|x_i \in R_m)\)</span>, where <span class="math notranslate nohighlight">\(\{R_1, \cdots, R_M\}\)</span> is the partition of a given tree and the final prediction of a sample is defined as</p>
<div class="math notranslate nohighlight">
\[f(x) = \sum_{m=1}^{M} c_m I(x\in R_m)\]</div>
<ul>
<li><p>Assume there are <span class="math notranslate nohighlight">\(d\)</span> feasures <span class="math notranslate nohighlight">\(x = (x_1, \cdots, x_d)\)</span> and the tree is split on the <span class="math notranslate nohighlight">\(j\)</span>th feasure (<span class="math notranslate nohighlight">\(j \in \{1, \cdots, d\}\)</span>), split point <span class="math notranslate nohighlight">\(s\in R\)</span>, then the partition based on <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(s\)</span>:</p>
<div class="math notranslate nohighlight">
\[R_1(j, s) = \{x|x_j\le s\}\]</div>
<div class="math notranslate nohighlight">
\[R_2(j, s) = \{x|x_j &gt; s\}\]</div>
</li>
<li><p>For each splitting variable <span class="math notranslate nohighlight">\(x_j\)</span> and split point <span class="math notranslate nohighlight">\(s\)</span>,</p>
<div class="math notranslate nohighlight">
\[\hat{c}_1(j, s) = avg(y_i|x_i\in R_1(j, s))\]</div>
<div class="math notranslate nohighlight">
\[\hat{c}_2(j, s) = avg(y_i|x_i\in R_2(j, s))\]</div>
</li>
<li><p>Find <span class="math notranslate nohighlight">\(j, s\)</span> by minimizing loss</p>
<div class="math notranslate nohighlight">
\[L(j,s)= \sum_{i:x_i\in R_1(j, s)}(y_i - \hat{c}_1(j, s))^2 + \sum_{i:x_i\in R_2(j, s)}(y_i - \hat{c}_2(j, s))^2\]</div>
</li>
</ul>
</li>
<li><p>Overcome the potential risk of over-fitting (see regularization parameters below)</p>
<ul class="simple">
<li><p>Trees can easily get over-fit if we do enough splitting</p></li>
<li><p>CART uses number of terminal nodes</p></li>
<li><p>Another option is tree depth</p></li>
</ul>
</li>
<li><p>Complexity of a tree</p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(|T | = M\)</span> denote the number of terminal nodes in <span class="math notranslate nohighlight">\(T\)</span> , which is used to measure the complexity of a tree</p></li>
<li><p>Finding the optimal binary tree of a given complexity is computationally intractable</p>
<ul>
<li><p>Use <strong>greedy algorithm</strong>: build the tree one node at a time, without any planning ahead</p></li>
</ul>
</li>
<li><p>Control the complexity of a tree (i.e. avoid over-fitting)</p>
<ul>
<li><p>limit max depth of tree</p></li>
<li><p>require all leaf nodes contain a minimum number of points</p></li>
<li><p>require a node have at least a certain number of data points to split</p></li>
<li><p>do <strong>backward pruning</strong> (the approach of CART (Breiman et al 1984)</p>
<ul>
<li><p>Build a really big tree (e.g. until all regions have <span class="math notranslate nohighlight">\(\le\)</span> 5 points).</p></li>
<li><p>“<strong>Prune</strong>” the tree back greedily all the way to the root, assessing performance on validation</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="classification-tree">
<h3>Classification tree<a class="headerlink" href="#classification-tree" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Similar to regression, the predicted classification for node <span class="math notranslate nohighlight">\(m\)</span> is just the class gives the highest probability, thus the most frequent class, in that node <span class="math notranslate nohighlight">\(m\)</span></p>
<div class="math notranslate nohighlight">
\[k(m) = \text{argmax}_k\hat{p}_{mk}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\hat{p}_{mk} = \frac{1}{N_m}\sum_{i: x_i\in R_m}I(y_i = k)\]</div>
</li>
<li><p>The missclassification rate on node <span class="math notranslate nohighlight">\(m\)</span> is <span class="math notranslate nohighlight">\(1 - \hat{p}_{mk(m)}\)</span></p></li>
<li><p>Loss function</p>
<ul class="simple">
<li><p>0/1 loss: tractable for finding the best split but is not used usually, especially there are repeatedly splitting insteading of splitting once</p></li>
<li><p>Eeventually we want <span class="math notranslate nohighlight">\(pure\)</span> leaf node and it’s thus natrual to find splitting node and point in that node to minimizing the node impurity after splitting</p>
<ul>
<li><p><strong>Miss classification error</strong>: <span class="math notranslate nohighlight">\(1 - \hat{p}_{mk(m)}\)</span></p></li>
<li><p><strong>Gini index</strong>: <span class="math notranslate nohighlight">\(\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K \hat{p}^2_{mk}\)</span></p></li>
<li><p><strong>Entropy or deviance</strong> (= using information gain): <span class="math notranslate nohighlight">\(-\sum_{k=1}^K\hat{p}_{mk}\log \hat{p}_{mk}\)</span></p>
<ul>
<li><p>Gini and Entropy seem to be more effective, they push for more pure nodes, not just misclassification rate!</p></li>
<li><p>Gini impurity is slightly faster to compute, so it’s a good default. When they differe, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.
<img alt="输入图片描述" src="https://raw.githubusercontent.com/askming/picgo/master/Screen%20Shot%202019-09-28%20at%2016.18.01_20200608034625.png" /></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>How to split</p>
<ul>
<li><p>To find the split that minimize the <strong>weighted average of node impurities</strong></p>
<div class="math notranslate nohighlight">
\[N_L*Q(R_L) + N_R*Q(R_R)\]</div>
<p>where <span class="math notranslate nohighlight">\(R_L\)</span> and <span class="math notranslate nohighlight">\(R_R\)</span> are potential splitting, <span class="math notranslate nohighlight">\(N_L\)</span> and <span class="math notranslate nohighlight">\(N_R\)</span> are number of observations in left and right branch after splitting and <span class="math notranslate nohighlight">\(Q(\cdot)\)</span> is the impurity measure</p>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="trees-in-general">
<h3>Trees in General<a class="headerlink" href="#trees-in-general" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Missing value in feature</p>
<ul>
<li><p>Complete analysis</p></li>
<li><p>impute with feature mean</p></li>
<li><p>If categorical, set missing as a new category</p></li>
<li><p>Use <strong>surrogate split</strong></p>
<ul>
<li><p>For every internal node, form a list of surrogate features and split points; Goal is to approximate the original split as well as possible; Surrogates ordered by how well they approximate the original split.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Categorical feature</p>
<ul>
<li><p>Suppose we have a categorical feature with <span class="math notranslate nohighlight">\(q\)</span> possible values (unordered).</p></li>
<li><p>For binary classification <span class="math notranslate nohighlight">\(Y =\{0, 1\}\)</span>, there is an efficient algorithm</p>
<ul>
<li><p>Assign each category a number, the proportion of class 0.</p></li>
<li><p>Then find optimal split as though it were a numeric feature.</p></li>
<li><p>Proved to be equivalent to search over all splits in (Breiman et al. 1984).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Trees have to work much harder to capture linear relations</p></li>
<li><p>Trees make no use of geometry</p>
<ul>
<li><p>No inner products or distances</p></li>
<li><p>called a “nonmetric” method</p></li>
<li><p>Feature scale irrelevant</p></li>
</ul>
</li>
<li><p>Predictions are not continuous</p>
<ul>
<li><p>not so bad for classification</p></li>
<li><p>may not be desirable for regression</p></li>
</ul>
</li>
<li><p><strong>Regularization Hyperparameters</strong>: To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this is called regularization</p>
<ul>
<li><p><em>max_depth</em></p></li>
<li><p><em>min_samples_split</em>: the minimum number of samples a node must have before it can be split</p></li>
<li><p><em>min_samples_leaf</em>: the minimum number of samples a leaf node must have</p></li>
<li><p><em>min_weight_fraction_leaf</em>: same as min_samples_leaf but expressed as a fraction of the total number of weighted instances</p></li>
<li><p><em>max_leaf_nodes</em>: maximum number of leaf nodes</p></li>
<li><p><em>max_features</em>: maximum number of features that are evaluated for splitting at each node</p></li>
</ul>
</li>
<li><p><strong>Instability</strong></p>
<ul>
<li><p>Decision Trees love <em>orthogonal decision boundaries</em> (all splits are perpendicular to an axis), which makes them <strong>sensitive to training set rotation</strong>.</p>
<ul>
<li><p>One way to limit this problem is to use PCA, which often results in a better orientation of the training data.</p></li>
</ul>
</li>
<li><p>More generally, the main issue with Decision Trees is that they are very <strong>sensitive to small variations in the training data</strong>.</p>
<ul>
<li><p>Random Forests can limit this instability by averaging predictions over many trees</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="tree-pruning">
<h3>Tree Pruning<a class="headerlink" href="#tree-pruning" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Build a “big tree”</p>
<ul class="simple">
<li><p>Keep splitting until every node either has zero error OR has <span class="math notranslate nohighlight">\(C\)</span> (e.g. 5 or 1)  or fewer examples</p></li>
</ul>
</li>
<li><p>Pruning</p>
<ul class="simple">
<li><p>Consider an internal node <span class="math notranslate nohighlight">\(n\)</span>, to prune the subtree rooted at <span class="math notranslate nohighlight">\(n\)</span> so that eliminate all descendents of <span class="math notranslate nohighlight">\(n\)</span>  and <span class="math notranslate nohighlight">\(n\)</span> becomes a terminal node</p></li>
</ul>
</li>
<li><p>Cost Complexity Greedy Pruning</p>
<ul>
<li><p>Cost of complexity criterion with <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[C_{\alpha} = \hat{R}(T) +  \alpha|T|\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{R}(T)\)</span> quantifies the empirical risk and <span class="math notranslate nohighlight">\(|T|\)</span> is the complexity of a tree</p>
</li>
<li><p>Thus, <span class="math notranslate nohighlight">\(C_{\alpha}\)</span> trades off betweem empirical risk and complexity of a tree</p></li>
<li><p>Cost complexity pruning:</p>
<ul>
<li><p>For each <span class="math notranslate nohighlight">\(\alpha\)</span>, find the subtree <span class="math notranslate nohighlight">\(T ⊂ T_0\)</span> minimizing <span class="math notranslate nohighlight">\(C_{\alpha} (T)\)</span> (on training data).</p></li>
<li><p>Use cross validation to find the right choice of <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>We only need to try <span class="math notranslate nohighlight">\(N_{Int}\)</span> trees to find the final pruned tree <span class="math notranslate nohighlight">\(T\)</span>, where <span class="math notranslate nohighlight">\(N_{Int}\)</span> is the number of internal nodes of <span class="math notranslate nohighlight">\(T_0\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\mathcal{T}= \{T_0 ⊃T_1 ⊃T_2 ⊃···⊃T_{|N_{Int}|}\}\]</div>
</li>
<li><p>Those <span class="math notranslate nohighlight">\(N_{|Int|}\)</span> trees can be identified by repeatedly pruning the tree (remove leaf nodes of an internal node) and choosing the one that minimes <span class="math notranslate nohighlight">\(\hat{R}(T_i) - \hat{R}(T_{i+1})\)</span> in each step until all internal nodes are removed and just a single (leaf) node is left</p></li>
<li><p>Breiman et al. (1984) proved that</p>
<div class="math notranslate nohighlight">
\[\left\{\arg\min_{T⊂T_0} C_{\alpha}(T) | \alpha \ge 0\right\} ⊂ \mathcal{T}\]</div>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</div>
</div>
<hr class="docutils" />
<div class="section" id="bagging">
<h2>Bagging<a class="headerlink" href="#bagging" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Parallel ensemble method <span class="math notranslate nohighlight">\(\rightarrow\)</span> bagging <span class="math notranslate nohighlight">\(\rightarrow\)</span> random forest<a class="footnote-reference brackets" href="#ensemble-bagging" id="id1">1</a></p></li>
<li><p>Averaging independent prediction functions is preferred over single predictor as it reduces the variance of the prediction; however, in reality we don’t have multiple indepenent samples, instead we can use bootstrap to construct multiple samples for different predictors (<strong>bagging</strong> = bootstrap aggregation<a class="footnote-reference brackets" href="#pasting" id="id2">2</a>)</p></li>
<li><p>Bagging (for regression)</p>
<ul>
<li><p>Draw <span class="math notranslate nohighlight">\(B\)</span> bootstrap samples <span class="math notranslate nohighlight">\(D^1,...,D^B\)</span> from original data <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(\hat{f}_1, \hat{f}_2, \cdots, \hat{f}_B: \mathcal{X} \rightarrow \bf{R}\)</span> be the prediction functions for each set</p></li>
<li><p>Bagged prediction function is given as</p>
<div class="math notranslate nohighlight">
\[\hat{f}_{\text{bag}} = \frac{1}{B}\sum_{b=1}^B\hat{f}_b(x)\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\hat{f}_{\text{bag}}(x)\)</span> usually has smaller variance than individual predictor <span class="math notranslate nohighlight">\(\hat{f}_{b}(x)\)</span></p></li>
</ul>
</li>
<li><p>Out-of-Bag error estimation</p>
<ul>
<li><p>Each bagged predictor is trained on about 63% (e.g.) of the data. Remaining 37% are called <strong>out-of-bag (OOB)</strong> observations.</p>
<div class="math notranslate nohighlight">
\[\hat{f}_{OOB}(x_i) = \frac{1}{S_i}\sum_{b\in S_i} \hat{f}_b(x_1)\]</div>
<p>where <span class="math notranslate nohighlight">\(S_i = \{b|D^b \text{does not contain ith point}\}\)</span></p>
<ul class="simple">
<li><p>The OOB error is a good estimate of the test error.</p></li>
<li><p>OOB error is similar to cross validation error – both are computed on training set.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>General sentiment is that bagging helps most when</p>
<ul class="simple">
<li><p>Relatively unbiased base prediction functions</p></li>
<li><p>High variance / low stability</p></li>
</ul>
</li>
<li><p>Bootstrap samples are</p>
<ul class="simple">
<li><p>independent samples from the training set, but are <strong>not</strong> indepedendent samples from <span class="math notranslate nohighlight">\(P_{\mathcal{X×Y}}\)</span>.</p></li>
<li><p>This dependence limits the amount of variance reduction we can get.</p></li>
</ul>
</li>
</ul>
<div class="section" id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Main idea of random forest</p>
<ul>
<li><p>Use <strong>bagged decision trees</strong>, but modify the tree-growing procedure to reduce the correlation between trees.</p></li>
<li><p>Key step in random forests: When constructing <strong>each tree node</strong>, restrict choice of splitting variable to a randomly chosen subset of features of size <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p>Typically choose <span class="math notranslate nohighlight">\(m \approx \sqrt{p}\)</span>, where <span class="math notranslate nohighlight">\(p\)</span> is the number of features.</p></li>
<li><p>Can choose m using cross validation.</p></li>
</ul>
</li>
</ul>
<div class="section" id="defining-variable-importance-in-rf">
<h4>Defining variable importance in RF<a class="headerlink" href="#defining-variable-importance-in-rf" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p><strong>Accuracy based</strong> importance</p>
<ul class="simple">
<li><p>Permutation importance: mean decrease in classification accuracy after permuting <span class="math notranslate nohighlight">\(X_j\)</span> over all trees</p></li>
</ul>
</li>
<li><p><strong>Gini importance</strong></p>
<ul class="simple">
<li><p>Mean Gini gain produced by splitting <span class="math notranslate nohighlight">\(X_j\)</span> over all trees</p></li>
</ul>
</li>
</ol>
<ul class="simple">
<li><p>Importance is calculated in out-of-sample trees</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="ensemble-bagging"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Bagging describles the way the training model is generated; ensemble describes the resulting training model (how it looks like)*</p>
</dd>
<dt class="label" id="pasting"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Pasting: similar idea as bagging but uses sampling without replacement</p>
</dd>
</dl>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./DS_ML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Note-Stochasitic%20Gradient%20Descent.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Stochastic Gradient Decent (SGD)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../CS/Note-HTML%20basics.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">HTML Basics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Ming Yang<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>